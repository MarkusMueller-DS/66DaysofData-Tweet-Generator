{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "66Days_Tweet-generator.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "AUc4vG-d-LzC"
      ]
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xysjTO68-Eb1"
      },
      "source": [
        "# Tweet-generator for #66DaysofData\n",
        "\n",
        "In this notebook I will train a <a href=\"https://openai.com/blog/better-language-models/\">GTP-2</a> model with collected tweets. I collected almost 15000 tweets form the #66DayofData-challenge on Twitter. You can read about the process on my <a href=\"https://markusmueller-ds.github.io/portfolio/66days_analysis.html\">website</a>. The goal is to create a tweet generator based on this dataset.\n",
        "\n",
        "The creator of the <a href=\"https://github.com/minimaxir/gpt-2-simple\">gtp-2-simple</a> libary publisched a great <a href=\"https://minimaxir.com/2020/01/twitter-gpt2-bot/\">blog post</a> explaining the process of using GTP-2 to create a tweet generator. \n",
        "\n",
        "### What is GPT-2?\n",
        "> GPT-2 is a large transformer-based language model with 1.5 billion parameters, trained on a dataset of 8 million web pages (40GB text data).\n",
        "\n",
        "### Trainings-parameters\n",
        "25.04.2021\n",
        "- model: '124M'\n",
        "- steps: 2000\n",
        "- run_name: 'run1'\n",
        "\n",
        "Training time: 01:18\n",
        "Evaluation: avg loss =1.20\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pWsV35WqN8y6"
      },
      "source": [
        "## Imports"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "h39jWc9tAmXB",
        "outputId": "c6c8fea6-d07a-42e6-ee75-37b6fd8acbe5"
      },
      "source": [
        "%tensorflow_version 1.x\n",
        "!pip install -q gpt-2-simple\n",
        "import gpt_2_simple as gpt2\n",
        "from datetime import datetime"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "TensorFlow 1.x selected.\n",
            "WARNING:tensorflow:\n",
            "The TensorFlow contrib module will not be included in TensorFlow 2.0.\n",
            "For more information, please see:\n",
            "  * https://github.com/tensorflow/community/blob/master/rfcs/20180907-contrib-sunset.md\n",
            "  * https://github.com/tensorflow/addons\n",
            "  * https://github.com/tensorflow/io (for I/O related ops)\n",
            "If you depend on functionality not listed there, please file an issue.\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MeajDRFCEOEE",
        "outputId": "4425ba03-a0d2-48d1-eb5e-f53dff644ffe"
      },
      "source": [
        "gpt2.mount_gdrive()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LuNZUOqWBW76",
        "outputId": "58c80d74-9f54-49b6-b2e0-b5790af09ddc"
      },
      "source": [
        "# check gpu\n",
        "# best case: Nvidia P100 GPU\n",
        "!nvidia-smi"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Sun Apr 25 08:24:43 2021       \n",
            "+-----------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 465.19.01    Driver Version: 460.32.03    CUDA Version: 11.2     |\n",
            "|-------------------------------+----------------------+----------------------+\n",
            "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
            "|                               |                      |               MIG M. |\n",
            "|===============================+======================+======================|\n",
            "|   0  Tesla T4            Off  | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   63C    P8    11W /  70W |      0MiB / 15109MiB |      0%      Default |\n",
            "|                               |                      |                  N/A |\n",
            "+-------------------------------+----------------------+----------------------+\n",
            "                                                                               \n",
            "+-----------------------------------------------------------------------------+\n",
            "| Processes:                                                                  |\n",
            "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
            "|        ID   ID                                                   Usage      |\n",
            "|=============================================================================|\n",
            "|  No running processes found                                                 |\n",
            "+-----------------------------------------------------------------------------+\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AUc4vG-d-LzC"
      },
      "source": [
        "## Load and prepare data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oGarc26A95pS"
      },
      "source": [
        "import numpy as np\n",
        "import pandas as pd"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uUBbFmKV-XK7"
      },
      "source": [
        "data = pd.read_csv('/content/drive/MyDrive/66Days-Generator/finalFrame.csv')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 711
        },
        "id": "qqJxSfvR-f4Q",
        "outputId": "a2b29d3e-771f-42e8-ebbc-71b24c5d3997"
      },
      "source": [
        "data"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>tweet_id</th>\n",
              "      <th>user_id</th>\n",
              "      <th>user_name</th>\n",
              "      <th>created_at</th>\n",
              "      <th>full_text</th>\n",
              "      <th>retweets</th>\n",
              "      <th>favorite</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>1299601482749181952</td>\n",
              "      <td>1292469347370360839</td>\n",
              "      <td>DuckPython</td>\n",
              "      <td>2020-08-29 06:55:13+00:00</td>\n",
              "      <td>@KenJee_DS looking forward to #66DaysOfData</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1299734773456203777</td>\n",
              "      <td>1159830350102781953</td>\n",
              "      <td>KenJee_DS</td>\n",
              "      <td>2020-08-29 15:44:52+00:00</td>\n",
              "      <td>Very excited to announce the #66daysofdata ini...</td>\n",
              "      <td>51</td>\n",
              "      <td>269</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>1299735515923505153</td>\n",
              "      <td>719854244</td>\n",
              "      <td>Sachin_g_here</td>\n",
              "      <td>2020-08-29 15:47:49+00:00</td>\n",
              "      <td>@KenJee_DS Looking fwd to #66Daysofdata</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>1299735809004769282</td>\n",
              "      <td>1001046433695285249</td>\n",
              "      <td>gautham53814486</td>\n",
              "      <td>2020-08-29 15:48:59+00:00</td>\n",
              "      <td>Let’s start #66daysofdata https://t.co/IPm1WhHaHB</td>\n",
              "      <td>0</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>1299736210575769605</td>\n",
              "      <td>1652520728</td>\n",
              "      <td>khudiamayankino</td>\n",
              "      <td>2020-08-29 15:50:35+00:00</td>\n",
              "      <td>@KenJee_DS count me in #66daysofdata</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>17241</th>\n",
              "      <td>1381306986067750918</td>\n",
              "      <td>731856877139558400</td>\n",
              "      <td>ABYA80</td>\n",
              "      <td>2021-04-11 18:03:43+00:00</td>\n",
              "      <td>R2: #66daysofdata with @KenJee_DS \\n\\nDay 27: ...</td>\n",
              "      <td>1</td>\n",
              "      <td>14</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>17242</th>\n",
              "      <td>1381326847527550983</td>\n",
              "      <td>324583975</td>\n",
              "      <td>georgekanellos</td>\n",
              "      <td>2021-04-11 19:22:38+00:00</td>\n",
              "      <td>Days 16-18(R2) of #66daysofdata by @KenJee_DS\\...</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>17243</th>\n",
              "      <td>1381336589641646083</td>\n",
              "      <td>1300492664308146176</td>\n",
              "      <td>MarkusM99098101</td>\n",
              "      <td>2021-04-11 20:01:21+00:00</td>\n",
              "      <td>Day 40 of  #66DaysOfData r2:\\n\\nread the first...</td>\n",
              "      <td>0</td>\n",
              "      <td>5</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>17244</th>\n",
              "      <td>1381338886241157124</td>\n",
              "      <td>1282311789464760321</td>\n",
              "      <td>HeqiqetEhmedova</td>\n",
              "      <td>2021-04-11 20:10:29+00:00</td>\n",
              "      <td>Day 4 of #100DaysOfCode ; #66daysofdata \\n   ✔...</td>\n",
              "      <td>10</td>\n",
              "      <td>5</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>17245</th>\n",
              "      <td>1381343032751046658</td>\n",
              "      <td>32453704</td>\n",
              "      <td>anyachocolat</td>\n",
              "      <td>2021-04-11 20:26:57+00:00</td>\n",
              "      <td>Day 14 of #66DaysOfData:\\n- Skillfactory cours...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>17246 rows × 7 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "                  tweet_id              user_id  ... retweets favorite\n",
              "0      1299601482749181952  1292469347370360839  ...        0        1\n",
              "1      1299734773456203777  1159830350102781953  ...       51      269\n",
              "2      1299735515923505153            719854244  ...        0        1\n",
              "3      1299735809004769282  1001046433695285249  ...        0        2\n",
              "4      1299736210575769605           1652520728  ...        0        1\n",
              "...                    ...                  ...  ...      ...      ...\n",
              "17241  1381306986067750918   731856877139558400  ...        1       14\n",
              "17242  1381326847527550983            324583975  ...        0        1\n",
              "17243  1381336589641646083  1300492664308146176  ...        0        5\n",
              "17244  1381338886241157124  1282311789464760321  ...       10        5\n",
              "17245  1381343032751046658             32453704  ...        0        0\n",
              "\n",
              "[17246 rows x 7 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QG3OwzFu-hlk",
        "outputId": "cae26911-2be8-49af-d62a-f2e24aa2e857"
      },
      "source": [
        "data.duplicated('tweet_id').sum()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "2500"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qvHdv2aS-t5e"
      },
      "source": [
        "# drop duplicates\n",
        "data = data.drop_duplicates(subset=['tweet_id'], keep='last')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IcMOqz8X-0eH"
      },
      "source": [
        "# frop unrelevant columns\n",
        "data = data[['full_text']]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QMx4pAPu_NNF"
      },
      "source": [
        "data.reset_index(inplace=True, drop=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 419
        },
        "id": "fR-AfUfx_dZP",
        "outputId": "48221af0-6723-4b75-8805-6a3ea99766a2"
      },
      "source": [
        "data"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>full_text</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>@KenJee_DS looking forward to #66DaysOfData</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>Very excited to announce the #66daysofdata ini...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>@KenJee_DS Looking fwd to #66Daysofdata</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>Let’s start #66daysofdata https://t.co/IPm1WhHaHB</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>@KenJee_DS count me in #66daysofdata</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>14741</th>\n",
              "      <td>R2: #66daysofdata with @KenJee_DS \\n\\nDay 27: ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>14742</th>\n",
              "      <td>Days 16-18(R2) of #66daysofdata by @KenJee_DS\\...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>14743</th>\n",
              "      <td>Day 40 of  #66DaysOfData r2:\\n\\nread the first...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>14744</th>\n",
              "      <td>Day 4 of #100DaysOfCode ; #66daysofdata \\n   ✔...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>14745</th>\n",
              "      <td>Day 14 of #66DaysOfData:\\n- Skillfactory cours...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>14746 rows × 1 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "                                               full_text\n",
              "0            @KenJee_DS looking forward to #66DaysOfData\n",
              "1      Very excited to announce the #66daysofdata ini...\n",
              "2                @KenJee_DS Looking fwd to #66Daysofdata\n",
              "3      Let’s start #66daysofdata https://t.co/IPm1WhHaHB\n",
              "4                   @KenJee_DS count me in #66daysofdata\n",
              "...                                                  ...\n",
              "14741  R2: #66daysofdata with @KenJee_DS \\n\\nDay 27: ...\n",
              "14742  Days 16-18(R2) of #66daysofdata by @KenJee_DS\\...\n",
              "14743  Day 40 of  #66DaysOfData r2:\\n\\nread the first...\n",
              "14744  Day 4 of #100DaysOfCode ; #66daysofdata \\n   ✔...\n",
              "14745  Day 14 of #66DaysOfData:\\n- Skillfactory cours...\n",
              "\n",
              "[14746 rows x 1 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XVgLKk4Y_kvp"
      },
      "source": [
        "# remove new line char\n",
        "data.full_text = data.full_text.replace(r'\\n','', regex=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 419
        },
        "id": "otq7sSJZ_5E4",
        "outputId": "41a6433b-e680-44e1-d2ba-ce4a1367ee66"
      },
      "source": [
        "data"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>full_text</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>@KenJee_DS looking forward to #66DaysOfData</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>Very excited to announce the #66daysofdata ini...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>@KenJee_DS Looking fwd to #66Daysofdata</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>Let’s start #66daysofdata https://t.co/IPm1WhHaHB</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>@KenJee_DS count me in #66daysofdata</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>14741</th>\n",
              "      <td>R2: #66daysofdata with @KenJee_DS Day 27: Had ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>14742</th>\n",
              "      <td>Days 16-18(R2) of #66daysofdata by @KenJee_DSF...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>14743</th>\n",
              "      <td>Day 40 of  #66DaysOfData r2:read the first sec...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>14744</th>\n",
              "      <td>Day 4 of #100DaysOfCode ; #66daysofdata    ✔️D...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>14745</th>\n",
              "      <td>Day 14 of #66DaysOfData:- Skillfactory course-...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>14746 rows × 1 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "                                               full_text\n",
              "0            @KenJee_DS looking forward to #66DaysOfData\n",
              "1      Very excited to announce the #66daysofdata ini...\n",
              "2                @KenJee_DS Looking fwd to #66Daysofdata\n",
              "3      Let’s start #66daysofdata https://t.co/IPm1WhHaHB\n",
              "4                   @KenJee_DS count me in #66daysofdata\n",
              "...                                                  ...\n",
              "14741  R2: #66daysofdata with @KenJee_DS Day 27: Had ...\n",
              "14742  Days 16-18(R2) of #66daysofdata by @KenJee_DSF...\n",
              "14743  Day 40 of  #66DaysOfData r2:read the first sec...\n",
              "14744  Day 4 of #100DaysOfCode ; #66daysofdata    ✔️D...\n",
              "14745  Day 14 of #66DaysOfData:- Skillfactory course-...\n",
              "\n",
              "[14746 rows x 1 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WI5POtxoClGX"
      },
      "source": [
        "# save file\n",
        "data.to_csv('finalData.csv', index=False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TPDnBs-2CsYl"
      },
      "source": [
        "final_data = pd.read_csv('/content/finalData.csv')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 419
        },
        "id": "y2o_lMGAC1xo",
        "outputId": "1dfd1bc0-6732-4231-963b-031945916b35"
      },
      "source": [
        "final_data"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>full_text</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>@KenJee_DS looking forward to #66DaysOfData</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>Very excited to announce the #66daysofdata ini...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>@KenJee_DS Looking fwd to #66Daysofdata</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>Let’s start #66daysofdata https://t.co/IPm1WhHaHB</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>@KenJee_DS count me in #66daysofdata</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>14743</th>\n",
              "      <td>R2: #66daysofdata with @KenJee_DS Day 27: Had ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>14744</th>\n",
              "      <td>Days 16-18(R2) of #66daysofdata by @KenJee_DSF...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>14745</th>\n",
              "      <td>Day 40 of  #66DaysOfData r2:read the first sec...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>14746</th>\n",
              "      <td>Day 4 of #100DaysOfCode ; #66daysofdata    ✔️D...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>14747</th>\n",
              "      <td>Day 14 of #66DaysOfData:- Skillfactory course-...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>14748 rows × 1 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "                                               full_text\n",
              "0            @KenJee_DS looking forward to #66DaysOfData\n",
              "1      Very excited to announce the #66daysofdata ini...\n",
              "2                @KenJee_DS Looking fwd to #66Daysofdata\n",
              "3      Let’s start #66daysofdata https://t.co/IPm1WhHaHB\n",
              "4                   @KenJee_DS count me in #66daysofdata\n",
              "...                                                  ...\n",
              "14743  R2: #66daysofdata with @KenJee_DS Day 27: Had ...\n",
              "14744  Days 16-18(R2) of #66daysofdata by @KenJee_DSF...\n",
              "14745  Day 40 of  #66DaysOfData r2:read the first sec...\n",
              "14746  Day 4 of #100DaysOfCode ; #66daysofdata    ✔️D...\n",
              "14747  Day 14 of #66DaysOfData:- Skillfactory course-...\n",
              "\n",
              "[14748 rows x 1 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UZhnEmtYGrn2"
      },
      "source": [
        "gpt2.encode_csv('/content/drive/MyDrive/66Days-Generator/finalData.csv')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6_Y7r-znCChy"
      },
      "source": [
        "## Train GPT-2\n",
        "I used the 124M base model. There are more performant modeks but hty use more disk space and are more suitable for longer texts, which is not the case for me.\n",
        "- Other models: 355M, 774M and 1558M"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "t4JtkrItAGKO",
        "outputId": "04880abb-3caf-488a-80bb-d353e22ddbec"
      },
      "source": [
        "# downlad model\n",
        "gpt2.download_gpt2(model_name=\"124M\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Fetching checkpoint: 1.05Mit [00:00, 242Mit/s]                                                      \n",
            "Fetching encoder.json: 1.05Mit [00:00, 4.91Mit/s]\n",
            "Fetching hparams.json: 1.05Mit [00:00, 507Mit/s]                                                    \n",
            "Fetching model.ckpt.data-00000-of-00001: 498Mit [00:11, 42.2Mit/s]                                  \n",
            "Fetching model.ckpt.index: 1.05Mit [00:00, 334Mit/s]                                                \n",
            "Fetching model.ckpt.meta: 1.05Mit [00:00, 7.97Mit/s]\n",
            "Fetching vocab.bpe: 1.05Mit [00:00, 6.05Mit/s]\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y6xhhDYgP317"
      },
      "source": [
        "The following code will finetune the GPT-2 model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9p5Ujx14DDrl",
        "outputId": "48cc586f-0acc-4330-e65b-147694003a0a"
      },
      "source": [
        "sess = gpt2.start_tf_sess()\n",
        "\n",
        "gpt2.finetune(sess,\n",
        "              dataset='/content/drive/MyDrive/66Days-Generator/csv_encoded.txt',\n",
        "              model_name='124M',\n",
        "              steps=2000,\n",
        "              restore_from='fresh',\n",
        "              run_name='run1',\n",
        "              print_every=10,\n",
        "              sample_every=500,\n",
        "              save_every=500\n",
        "              )"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.7/dist-packages/gpt_2_simple/src/sample.py:17: where (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
            "Loading checkpoint models/124M/model.ckpt\n",
            "INFO:tensorflow:Restoring parameters from models/124M/model.ckpt\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\r  0%|          | 0/1 [00:00<?, ?it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Loading dataset...\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 1/1 [00:04<00:00,  4.61s/it]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "dataset has 979860 tokens\n",
            "Training...\n",
            "[10 | 29.27] loss=2.89 avg=2.89\n",
            "[20 | 52.25] loss=2.78 avg=2.83\n",
            "[30 | 75.78] loss=2.62 avg=2.76\n",
            "[40 | 98.88] loss=2.63 avg=2.73\n",
            "[50 | 121.79] loss=2.56 avg=2.69\n",
            "[60 | 144.93] loss=2.63 avg=2.68\n",
            "[70 | 168.11] loss=2.44 avg=2.65\n",
            "[80 | 191.19] loss=2.51 avg=2.63\n",
            "[90 | 214.29] loss=2.37 avg=2.60\n",
            "[100 | 237.45] loss=2.44 avg=2.58\n",
            "[110 | 260.60] loss=2.47 avg=2.57\n",
            "[120 | 283.76] loss=2.33 avg=2.55\n",
            "[130 | 306.92] loss=2.41 avg=2.54\n",
            "[140 | 330.08] loss=2.44 avg=2.53\n",
            "[150 | 353.21] loss=2.28 avg=2.51\n",
            "[160 | 376.35] loss=2.42 avg=2.51\n",
            "[170 | 399.48] loss=2.42 avg=2.50\n",
            "[180 | 422.61] loss=2.35 avg=2.49\n",
            "[190 | 445.73] loss=2.30 avg=2.48\n",
            "[200 | 468.84] loss=2.43 avg=2.48\n",
            "[210 | 491.96] loss=2.39 avg=2.47\n",
            "[220 | 515.08] loss=2.19 avg=2.46\n",
            "[230 | 538.22] loss=2.21 avg=2.45\n",
            "[240 | 561.36] loss=2.37 avg=2.44\n",
            "[250 | 584.49] loss=2.20 avg=2.43\n",
            "[260 | 607.64] loss=2.17 avg=2.42\n",
            "[270 | 630.74] loss=2.46 avg=2.42\n",
            "[280 | 653.83] loss=2.08 avg=2.41\n",
            "[290 | 676.92] loss=2.14 avg=2.40\n",
            "[300 | 700.03] loss=2.35 avg=2.40\n",
            "[310 | 723.12] loss=2.23 avg=2.39\n",
            "[320 | 746.23] loss=2.12 avg=2.38\n",
            "[330 | 769.38] loss=2.07 avg=2.37\n",
            "[340 | 792.55] loss=2.23 avg=2.37\n",
            "[350 | 815.70] loss=2.21 avg=2.36\n",
            "[360 | 838.83] loss=2.11 avg=2.35\n",
            "[370 | 861.97] loss=2.11 avg=2.34\n",
            "[380 | 885.11] loss=2.15 avg=2.34\n",
            "[390 | 908.24] loss=2.22 avg=2.33\n",
            "[400 | 931.32] loss=2.10 avg=2.33\n",
            "[410 | 954.45] loss=2.16 avg=2.32\n",
            "[420 | 977.61] loss=2.00 avg=2.31\n",
            "[430 | 1000.76] loss=2.07 avg=2.31\n",
            "[440 | 1023.92] loss=1.87 avg=2.29\n",
            "[450 | 1047.06] loss=2.05 avg=2.29\n",
            "[460 | 1070.15] loss=1.93 avg=2.28\n",
            "[470 | 1093.24] loss=2.11 avg=2.27\n",
            "[480 | 1116.36] loss=1.92 avg=2.26\n",
            "[490 | 1139.50] loss=1.86 avg=2.25\n",
            "[500 | 1162.63] loss=1.85 avg=2.24\n",
            "Saving checkpoint/run1/model-500\n",
            "======== SAMPLE 1 ========\n",
            ".\n",
            "Day 23: https://t.co/fL2R8NvWVnI The Ultimate #Titanic #Database #DataScience #Import Data#66daysofdata #DataAnalytics #DeepLearning #pythonprogramming #DataScience #monetization #AI #coding #programmingtwitter #Yahoo’s API<|endoftext|>\n",
            "<|startoftext|>Day 25 of #66DaysofData - Finished the Housing Prices Competition 🌩️<|endoftext|>\n",
            "<|startoftext|>Day 24 of #66daysofdata !- I went through the 3rd chapter of \"Intermediate Machine Learning\", by Aurélien Géron, a must for anyone new to Deep Learning !- I reviewed my book - \"Browsing the Brain - Hands-on Machine Learning\" @KenJee_DS<|endoftext|>\n",
            "<|startoftext|>Day 25 of #66daysofdata by Ken attended a conference on Data Engineering https://t.co/4V5NgVQXt9<|endoftext|>\n",
            "<|startoftext|>Day 15 of #66daysofdata Today I will continue with the 'Importing Data into MySQL' project by @datacamp. I have to admit it was a challenge but I was able to write scripts for the sake of the challenge...<|endoftext|>\n",
            "<|startoftext|>Day 25 of #66DaysOfDatacompleted some exercises on Python on using 'py4e' to display a chart. #DataCulture https://t.co/N8X0nEtXJg<|endoftext|>\n",
            "<|startoftext|>D.9 #66daysofdata Read the first chapter of \"The hundred-page machine learning book\". I've been watching videos on deep learning and how DBM can be used in Artificial Neural Networks.https://t.co/NlP1b3hRzq<|endoftext|>\n",
            "<|startoftext|>Day 7 of #66DaysofData! Today I’m going to use R for an interview. A great guide for Data Science Interviews. I just finished my first few videos. https://t.co/jIhBHQzYW6 #R<|endoftext|>\n",
            "<|startoftext|>Day 25 of #66daysofdata with @KenJee_DS Today I attended @YouTube's \"Introduction to Machine Learning with Scratch\" @KenJee_DS I have a hard time putting the same skills into some basic stuff... I know I can take more from it than I do from the actual skill level but I know I am missing the big picture.<|endoftext|>\n",
            "<|startoftext|>Day 27 of #Round2 #66daysofdata  with @KenJee_DS  Continued to explore the concepts of SQL, learned more about using the CASE WHEN operator on strings and how to use CASE WHEN.<|endoftext|>\n",
            "<|startoftext|>Day 26 of #66DaysOfData! Today I will continue to learn some SQL from Tableau.I started reviewing all the concepts of databases using JOINS on @Kaggle. https://t.co/5F5DgKtLW1g https://t.co/Vr6d6T0jK4<|endoftext|>\n",
            "<|startoftext|>Day 22: #66daysofdata - Did an exploratory data analysis project on SQLite - Studied about Tableau fundamentals in Python - Learned about SQL and SQLITE - Learnt how to create tables and how to insert and delete data - Learned about GROUP BY operator and JOINS!<|endoftext|>\n",
            "<|startoftext|>D25. #66DaysofData - Started SQL on Dataquest  on April 1st.Finished working on the EDA project from today.Read about the basic and advanced SQL syntax. #DataScience #Analytics<|endoftext|>\n",
            "<|startoftext|>Day 23 of #66daysofdataI have revised the basics of #NeuralNetworks (the basics are very similar to #NLP and its derivatives) by practicing some #DeepLearning algorithms (and in general applying them).https://t.co/p0cIqXgPtF<|endoftext|>\n",
            "<|startoftext|>#Day1 of #66DaysOfData with @KenJee_DS Today I am reading Chapter 8: Creating Neural Network. https://t.co/3t7Kgw\n",
            "\n",
            "[510 | 1198.77] loss=2.04 avg=2.24\n",
            "[520 | 1221.90] loss=2.03 avg=2.23\n",
            "[530 | 1244.96] loss=1.93 avg=2.23\n",
            "[540 | 1268.08] loss=2.05 avg=2.22\n",
            "[550 | 1291.20] loss=2.03 avg=2.22\n",
            "[560 | 1314.35] loss=1.82 avg=2.21\n",
            "[570 | 1337.50] loss=2.04 avg=2.20\n",
            "[580 | 1360.64] loss=1.75 avg=2.19\n",
            "[590 | 1383.77] loss=1.97 avg=2.19\n",
            "[600 | 1406.89] loss=1.80 avg=2.18\n",
            "[610 | 1430.00] loss=2.07 avg=2.18\n",
            "[620 | 1453.13] loss=1.97 avg=2.17\n",
            "[630 | 1476.23] loss=1.85 avg=2.17\n",
            "[640 | 1499.30] loss=1.86 avg=2.16\n",
            "[650 | 1522.37] loss=1.92 avg=2.16\n",
            "[660 | 1545.48] loss=1.53 avg=2.14\n",
            "[670 | 1568.59] loss=1.78 avg=2.13\n",
            "[680 | 1591.73] loss=1.72 avg=2.13\n",
            "[690 | 1614.85] loss=2.06 avg=2.13\n",
            "[700 | 1637.97] loss=1.75 avg=2.12\n",
            "[710 | 1661.09] loss=1.83 avg=2.11\n",
            "[720 | 1684.23] loss=1.88 avg=2.11\n",
            "[730 | 1707.35] loss=1.62 avg=2.10\n",
            "[740 | 1730.50] loss=1.77 avg=2.09\n",
            "[750 | 1753.64] loss=1.54 avg=2.08\n",
            "[760 | 1776.75] loss=1.84 avg=2.08\n",
            "[770 | 1799.88] loss=1.57 avg=2.07\n",
            "[780 | 1823.02] loss=1.53 avg=2.06\n",
            "[790 | 1846.13] loss=1.61 avg=2.05\n",
            "[800 | 1869.26] loss=1.51 avg=2.04\n",
            "[810 | 1892.38] loss=1.85 avg=2.04\n",
            "[820 | 1915.48] loss=1.48 avg=2.03\n",
            "[830 | 1938.58] loss=1.67 avg=2.02\n",
            "[840 | 1961.70] loss=1.50 avg=2.01\n",
            "[850 | 1984.80] loss=1.83 avg=2.01\n",
            "[860 | 2007.90] loss=1.77 avg=2.00\n",
            "[870 | 2031.00] loss=1.70 avg=2.00\n",
            "[880 | 2054.10] loss=1.71 avg=1.99\n",
            "[890 | 2077.19] loss=1.42 avg=1.98\n",
            "[900 | 2100.28] loss=1.84 avg=1.98\n",
            "[910 | 2123.37] loss=1.64 avg=1.98\n",
            "[920 | 2146.45] loss=1.47 avg=1.97\n",
            "[930 | 2169.54] loss=1.58 avg=1.96\n",
            "[940 | 2192.63] loss=1.43 avg=1.95\n",
            "[950 | 2215.70] loss=1.78 avg=1.95\n",
            "[960 | 2238.78] loss=1.41 avg=1.94\n",
            "[970 | 2261.85] loss=1.40 avg=1.93\n",
            "[980 | 2284.92] loss=1.31 avg=1.92\n",
            "[990 | 2307.98] loss=1.53 avg=1.92\n",
            "[1000 | 2331.04] loss=1.58 avg=1.91\n",
            "Saving checkpoint/run1/model-1000\n",
            "WARNING:tensorflow:From /tensorflow-1.15.2/python3.7/tensorflow_core/python/training/saver.py:963: remove_checkpoint (from tensorflow.python.training.checkpoint_management) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use standard file APIs to delete files with this prefix.\n",
            "======== SAMPLE 1 ========\n",
            "PractualSQLList project with YOLO - using it for training our perceptron models, and then comparing the models. The final model will be announced after extensive study!https://t.co/YpB4N1lLhK<|endoftext|>\n",
            "<|startoftext|>Day 25 of #66DaysOfData with @KenJee_DS.Continued with the Housing Dataset from Kaggle. Brief overviews of the software in SQL.Also watched the video of @KenJee_DS \"A brief introduction to data science project\". Interesting video.<|endoftext|>\n",
            "<|startoftext|>Day 23 of #66daysofdata Today I had time to finish the Kaggle course, Linear Discriminant Analysis (LDA). https://t.co/nhxK2h2n7B<|endoftext|>\n",
            "<|startoftext|>Day 25 of #66daysofdata -I revised my Statistics for Data Science Kaggle lesson 2, learned about the basic intuition behind the lesson.  I went through several lessons on SVM and Naive Bayes Classifier.<|endoftext|>\n",
            "<|startoftext|>Day 25 of #66DaysOfDataSQL Revisit: Types of Sub-Strings and Strings in SQL Server@KenJee_DShttps://t.co/3Wt2LlW5j3<|endoftext|>\n",
            "<|startoftext|>Day 12 of #66daysofdata :Didn't do much today, I worked on a kaggle project for the time being and then I watched a couple of videos on https://t.co/mCfj2L0aN<|endoftext|>\n",
            "<|startoftext|>Day 24- Watched some video tutorial video in @kaggle- went through examples of how to use @emilykinsey 's clever storytelling video to refresh your memory about data analysis. https://t.co/j3hLJwX5vJ<|endoftext|>\n",
            "<|startoftext|>Day 24 V_2 of #66DaysOfData with @KenJee_DS(2) :- Watched and learned 2 videos from the https://t.co/Jw2o2VyqT video- Started with @HackerRank and followed by @JackRaifer's (and others') \"Hacking the App Market\" serieshttps://t.co/5Bc0x2Td4U<|endoftext|>\n",
            "<|startoftext|>Day 18 of #66daysofdata by @KenJee_DS started with  Dimensionality Reduction. This is a pretty cool application of Linear Classifier to improve a model's precision at low dimensions (less than 0.1). https://t.co/2lLmTfSqLh<|endoftext|>\n",
            "<|startoftext|>Day 12 of #66daysofdata - Today I continued study about feature scaling. The topic is: How much should a given feature be chosen from the image?<|endoftext|>\n",
            "<|startoftext|>day 18: As someone who's always looking at teaching small classes in my pet project, I couldn't think of any better way to start #66daysofdata. So I am creating a template for the above and the other 25 pages.#66DaysOfData<|endoftext|>\n",
            "<|startoftext|>10/66 #66daysofdata  Hi Everyone!I read a few of Chapter 2 (Hands-on Machine Learning with Scikit-Learn, TensorFlow) published by @OReillyMedia on @Medium . Very informative for the people who want to learn something from reading it in full. https://t.co/g3i8eHkqjF<|endoftext|>\n",
            "<|startoftext|>What is the most important type of feature for Image Import in Python? #100DaysOfCode #100DaysOfMLCode #100daysofcode #66daysofdata #MachineLearning #CodeNewbies #code #DigitalEngineering #code #100DaysOfCode #100DaysOfMLCode #Python #DEVCommunity #100DaysOfCode #AI #computerscience #ArtificialIntelligence #webdev https://t.co/fJfzQZjbVc<|endoftext|>\n",
            "<|startoftext|>Day 21 of #66daysofdataContinued my udemy python course. Learning more about Data Manipulation with Pandas and Data Comprehensions from @DataCamp<\n",
            "\n",
            "[1010 | 2365.62] loss=1.39 avg=1.90\n",
            "[1020 | 2388.76] loss=1.37 avg=1.89\n",
            "[1030 | 2411.88] loss=1.48 avg=1.89\n",
            "[1040 | 2434.98] loss=1.41 avg=1.88\n",
            "[1050 | 2458.08] loss=1.45 avg=1.87\n",
            "[1060 | 2481.16] loss=1.29 avg=1.86\n",
            "[1070 | 2504.21] loss=1.43 avg=1.86\n",
            "[1080 | 2527.28] loss=1.52 avg=1.85\n",
            "[1090 | 2550.35] loss=1.52 avg=1.85\n",
            "[1100 | 2573.44] loss=1.56 avg=1.84\n",
            "[1110 | 2596.54] loss=1.23 avg=1.83\n",
            "[1120 | 2619.67] loss=1.04 avg=1.82\n",
            "[1130 | 2642.79] loss=1.46 avg=1.82\n",
            "[1140 | 2665.89] loss=1.42 avg=1.81\n",
            "[1150 | 2688.98] loss=1.31 avg=1.80\n",
            "[1160 | 2712.04] loss=1.29 avg=1.80\n",
            "[1170 | 2735.11] loss=1.50 avg=1.79\n",
            "[1180 | 2758.17] loss=1.47 avg=1.79\n",
            "[1190 | 2781.24] loss=1.13 avg=1.78\n",
            "[1200 | 2804.32] loss=1.21 avg=1.77\n",
            "[1210 | 2827.42] loss=1.18 avg=1.76\n",
            "[1220 | 2850.53] loss=1.36 avg=1.76\n",
            "[1230 | 2873.65] loss=1.21 avg=1.75\n",
            "[1240 | 2896.78] loss=1.23 avg=1.74\n",
            "[1250 | 2919.91] loss=1.22 avg=1.73\n",
            "[1260 | 2943.08] loss=1.09 avg=1.73\n",
            "[1270 | 2966.22] loss=1.32 avg=1.72\n",
            "[1280 | 2989.35] loss=1.18 avg=1.71\n",
            "[1290 | 3012.46] loss=1.07 avg=1.70\n",
            "[1300 | 3035.55] loss=1.27 avg=1.70\n",
            "[1310 | 3058.63] loss=1.04 avg=1.69\n",
            "[1320 | 3081.69] loss=1.15 avg=1.68\n",
            "[1330 | 3104.76] loss=1.03 avg=1.67\n",
            "[1340 | 3127.82] loss=1.34 avg=1.67\n",
            "[1350 | 3150.90] loss=1.17 avg=1.66\n",
            "[1360 | 3174.01] loss=1.20 avg=1.65\n",
            "[1370 | 3197.15] loss=0.89 avg=1.64\n",
            "[1380 | 3220.30] loss=1.14 avg=1.64\n",
            "[1390 | 3243.44] loss=1.09 avg=1.63\n",
            "[1400 | 3266.57] loss=1.16 avg=1.62\n",
            "[1410 | 3289.70] loss=1.05 avg=1.62\n",
            "[1420 | 3312.82] loss=1.18 avg=1.61\n",
            "[1430 | 3335.96] loss=0.96 avg=1.60\n",
            "[1440 | 3359.06] loss=0.82 avg=1.59\n",
            "[1450 | 3382.17] loss=1.03 avg=1.58\n",
            "[1460 | 3405.24] loss=1.14 avg=1.58\n",
            "[1470 | 3428.30] loss=1.07 avg=1.57\n",
            "[1480 | 3451.38] loss=0.94 avg=1.56\n",
            "[1490 | 3474.45] loss=0.81 avg=1.55\n",
            "[1500 | 3497.52] loss=0.86 avg=1.55\n",
            "Saving checkpoint/run1/model-1500\n",
            "======== SAMPLE 1 ========\n",
            ", \"How to learn Python with Data Science\"https://t.co/aDU00s6J3f#DataScience #MachineLearning #Python #DataScience<|endoftext|>\n",
            "<|startoftext|>Day 9 of #66DaysOfData! \"Machine Learning\" by AndrewYNg on #Coursera.Part of the course:- Week5/Regularization: Logistic regression, decision trees and random forest.<|endoftext|>\n",
            "<|startoftext|>Day 3: 2nd round #66daysofdata - completed project of analysis - covered next ML algorithms- started EDA - still not done with it but interested to learn more about web scrapingIt's probably not the most interesting topic, but I can say it's important.https://t.co/A7sS2jEeZn<|endoftext|>\n",
            "<|startoftext|>Day 7: 2nd round #66daysofdata - completed project of data visualization on @HackerRank It's not sure if I'll continue, but i'm excited to try and watch how it goes!<|endoftext|>\n",
            "<|startoftext|>Day 3 of #66daysofdata.I used the time to brush up on some statistics.I learnt about the distributions and different means of doing the EDA. I also interacted with some datasets and played around with them.<|endoftext|>\n",
            "<|startoftext|>Day 3 of #66DaysOfData:- Started 'Data Analytics' by Jose Portilla- Learned more about Python in Private Quantopian<|endoftext|>\n",
            "<|startoftext|>I missed yesterday so I'm on Day 2 now!- Participated in #RandomIntelligence Experiment (SKU) on #kaggle using the COLLECTION fundai dataset on #kagglehttps://t.co/QmWqxW7R6I #DataScience  #DataAnalytics #DataScientist #66daysofdata  v2 @KenJee_DShttps://t.co/4s2WK7ztJZ<|endoftext|>\n",
            "<|startoftext|>Day 1 of #66daysofdata : Reviewing Multiple Regression: R2- Regression Model Read more about R2- Regression Model on @Codecademy https://t.co/P2NTb0RlSj.  This time around, I mainly learned about R2<|endoftext|>\n",
            "<|startoftext|>@KenJee_DS Thanks, I'll be happy if I re-post my progress too. I learned a lot still though! #66DaysOfData is officially over for me, because I managed to finish my #DataAnalytics book and start my Python journey.#DataScience#100DaysOfCode #Python #DataScientist #66daysofdata https://t.co/M6Vh1W9mM7<|endoftext|>\n",
            "<|startoftext|>Day 3 of #66daysofdata:Completed the week in a few #nflfastR theory sessions with @CoreyMSchafer. Never thought I'd be able to make something like this:1/3<|endoftext|>\n",
            "<|startoftext|>Day 3 of #66daysofdata!Watched a couple of videos on how to get a machine learning brush-on from @dataleaptech (https://t.co/Wd3PXg5tkq). Need to brush up on some basics in order to brush up on @zerotomasteryio and his amazing videos<|endoftext|>\n",
            "<|startoftext|>#66DaysOfData with @KenJee_DS round 2 day #28 and round 2 day #31:-&gt; Gave my day job interview as a data analyst and also went on a Data Engineering Explained podcast vid @KenJee_DS.It's definitely refreshing to go through his journey and hear his journey-level challenges and challenges that he has gone through.https://t.co/IgSIGJW3B3<|endoftext|>\n",
            "<|startoftext|>Day 27 of #66daysofdataStarted working on an assignment for a customer research organization.  Implementing Machine Learning Algorithm utilizing Ridge and Bayesian methods.#datascience #machinelearning #python<|endoftext|>\n",
            "<|startoftext|>Day 28: ML with R #66daysofdata<|endoftext|>\n",
            "<|startoftext|>Day 20 of #66daysofdata R2- Today was a light session.\n",
            "\n",
            "[1510 | 3532.13] loss=1.29 avg=1.54\n",
            "[1520 | 3555.28] loss=0.85 avg=1.53\n",
            "[1530 | 3578.41] loss=0.91 avg=1.53\n",
            "[1540 | 3601.52] loss=0.84 avg=1.52\n",
            "[1550 | 3624.59] loss=0.60 avg=1.51\n",
            "[1560 | 3647.65] loss=0.90 avg=1.50\n",
            "[1570 | 3670.71] loss=0.87 avg=1.49\n",
            "[1580 | 3693.75] loss=0.85 avg=1.48\n",
            "[1590 | 3716.81] loss=0.93 avg=1.47\n",
            "[1600 | 3739.86] loss=0.99 avg=1.47\n",
            "[1610 | 3762.97] loss=1.02 avg=1.46\n",
            "[1620 | 3786.09] loss=0.81 avg=1.46\n",
            "[1630 | 3809.19] loss=0.72 avg=1.45\n",
            "[1640 | 3832.31] loss=1.12 avg=1.44\n",
            "[1650 | 3855.46] loss=0.85 avg=1.43\n",
            "[1660 | 3878.61] loss=1.03 avg=1.43\n",
            "[1670 | 3901.78] loss=0.78 avg=1.42\n",
            "[1680 | 3924.93] loss=0.83 avg=1.41\n",
            "[1690 | 3948.08] loss=0.84 avg=1.41\n",
            "[1700 | 3971.21] loss=0.75 avg=1.40\n",
            "[1710 | 3994.35] loss=0.75 avg=1.39\n",
            "[1720 | 4017.48] loss=1.01 avg=1.39\n",
            "[1730 | 4040.62] loss=0.87 avg=1.38\n",
            "[1740 | 4063.73] loss=0.80 avg=1.37\n",
            "[1750 | 4086.85] loss=0.99 avg=1.37\n",
            "[1760 | 4109.98] loss=0.91 avg=1.36\n",
            "[1770 | 4133.09] loss=0.61 avg=1.35\n",
            "[1780 | 4156.20] loss=0.86 avg=1.35\n",
            "[1790 | 4179.31] loss=0.65 avg=1.34\n",
            "[1800 | 4202.42] loss=0.88 avg=1.33\n",
            "[1810 | 4225.53] loss=0.71 avg=1.33\n",
            "[1820 | 4248.67] loss=0.63 avg=1.32\n",
            "[1830 | 4271.81] loss=0.69 avg=1.31\n",
            "[1840 | 4294.94] loss=0.81 avg=1.31\n",
            "[1850 | 4318.06] loss=0.65 avg=1.30\n",
            "[1860 | 4341.17] loss=0.76 avg=1.29\n",
            "[1870 | 4364.30] loss=0.76 avg=1.28\n",
            "[1880 | 4387.44] loss=0.60 avg=1.28\n",
            "[1890 | 4410.57] loss=0.62 avg=1.27\n",
            "[1900 | 4433.67] loss=0.64 avg=1.26\n",
            "[1910 | 4456.77] loss=0.61 avg=1.25\n",
            "[1920 | 4479.89] loss=0.77 avg=1.25\n",
            "[1930 | 4502.99] loss=0.72 avg=1.24\n",
            "[1940 | 4526.09] loss=0.56 avg=1.23\n",
            "[1950 | 4549.22] loss=0.75 avg=1.23\n",
            "[1960 | 4572.34] loss=0.83 avg=1.22\n",
            "[1970 | 4595.46] loss=0.53 avg=1.22\n",
            "[1980 | 4618.57] loss=0.55 avg=1.21\n",
            "[1990 | 4641.72] loss=0.58 avg=1.20\n",
            "[2000 | 4664.85] loss=0.72 avg=1.20\n",
            "Saving checkpoint/run1/model-2000\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "u9KLqVeyIF2o",
        "outputId": "b47e426e-e2a1-4cd0-d37a-9e5cfac99039"
      },
      "source": [
        "# Zip run in checkpoint folder\n",
        "!zip -r /content/run1.zip /content/checkpoint/run1"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "  adding: content/checkpoint/run1/ (stored 0%)\n",
            "  adding: content/checkpoint/run1/model-2000.data-00000-of-00001 (deflated 7%)\n",
            "  adding: content/checkpoint/run1/model-2000.meta (deflated 91%)\n",
            "  adding: content/checkpoint/run1/counter (stored 0%)\n",
            "  adding: content/checkpoint/run1/model-2000.index (deflated 62%)\n",
            "  adding: content/checkpoint/run1/events.out.tfevents.1619339169.0cf9f22a178c (deflated 61%)\n",
            "  adding: content/checkpoint/run1/encoder.json (deflated 67%)\n",
            "  adding: content/checkpoint/run1/checkpoint (deflated 40%)\n",
            "  adding: content/checkpoint/run1/vocab.bpe (deflated 53%)\n",
            "  adding: content/checkpoint/run1/hparams.json (deflated 28%)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xSPHf7z6cm_z",
        "outputId": "f98c3546-de88-472d-a5ee-0756d6251c1b"
      },
      "source": [
        "!unzip /content/run1.zip"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Archive:  /content/run1.zip\n",
            "   creating: content/checkpoint/run1/\n",
            "  inflating: content/checkpoint/run1/model-2000.data-00000-of-00001  \n",
            "  inflating: content/checkpoint/run1/model-2000.meta  \n",
            " extracting: content/checkpoint/run1/counter  \n",
            "  inflating: content/checkpoint/run1/model-2000.index  \n",
            "  inflating: content/checkpoint/run1/events.out.tfevents.1619339169.0cf9f22a178c  \n",
            "  inflating: content/checkpoint/run1/encoder.json  \n",
            "  inflating: content/checkpoint/run1/checkpoint  \n",
            "  inflating: content/checkpoint/run1/vocab.bpe  \n",
            "  inflating: content/checkpoint/run1/hparams.json  \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MTth9AqEbsyv",
        "outputId": "2ad9823d-87fa-471e-c2f5-eddac264f4f4"
      },
      "source": [
        "gpt2.generate(sess, run_name='run1')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Ken Jee’s next step might be to start a data science book.  Not gonna lie. It's gonna take time.  But, ya, ya! :) #66daysofdata<|endoftext|>\n",
            "<|startoftext|>Day 25 of #66daysofdata:I've completed the Introduction to the Tidyverse course on Dataquest. It's one of my favorite parts of the course, it's good people I know and use Text Analysis and Data Wrangling for data science.<|endoftext|>\n",
            "<|startoftext|>Day 3 of #66daysofdata :I've completed the Pandas course on Kaggle! https://t.co/Zq2ubw2kYM<|endoftext|>\n",
            "<|startoftext|>Day 25: I had watched: Tutorial for Understanding and Visualizing Machine Learning in Python: https://t.co/yVZ9qJit2F #66daysofdata #datascience https://t.co/Zj7m9Z5zPs<|endoftext|>\n",
            "<|startoftext|>Day 25 of #66daysofdata:I finished the kaggle python course. I’m also going to start a data science one :)<|endoftext|>\n",
            "<|startoftext|>Day 25 of #66daysofdataSQL Revisit:SQL statements are not required when working with large strings. (char, aggregate, timestamp)• working with large strings in excel• using a dict to organise values in the dataframe• working with large strings in python<|endoftext|>\n",
            "<|startoftext|>Day 6 of #66daysofdataRead Chapter 3 of \"Hands-On Machine Learning\" which explored the use of RNNs and LSTMs in classification problems. @KenJee_DS<|endoftext|>\n",
            "<|startoftext|>Day 25 of #66daysofdata: Classification performance of COVID-19 confirmed that the model is correct. The model predictions are pretty good, even with multiple features in the model. The main issue is that the training time is shorter than the test time.#DataScience #MachineLearning https://t.co/Ibd8Cj9MpT<|endoftext|>\n",
            "<|startoftext|>Day 25 of #66daysofdata: Today, I started the second course of the Tensorflow Developer Certificate Specialisation @coursera, and volunteered for Houston Food Bank for ~ 3 hours while everybody is watching elections (:<|endoftext|>\n",
            "<|startoftext|>#100DaysOfCode #66daysofdata Day 1 (Sept 25) : - Spent about an hour learning data structures.- Learnt about NLP and computational vision.- Worked on a notebook about numerai (hours to build it).<|endoftext|>\n",
            "<|startoftext|>Day 25 of #66daysofdata: I started the 'Feature Engineering' course on Kaggle. I'm looking forward to applying this knowledge into my projects!<|endoftext|>\n",
            "<|startoftext|>Day 25 of #66daysofdata:Finished \"Intro to importing SQL data\" on Data Camp.<|endoftext|>\n",
            "<|startoftext|>Day 25 of #66daysofdata:  Today, I started  SQL Zoo, the first supervised learning model in which I will learn and apply some of the supervised learning principles from the book. @KenJee_DS<|endoftext|>\n",
            "<|startoftext|>Day 25 of #66daysofdata: Today, I finished Kaggle's Geospatial Analysis mini-course, I completed the assignment \"Using OOPs in Kaggle Compose\". It's a good course summary and a good reason to take this #DataScience course. https://t.co/YsAZqrteLQ<|endoftext|>\n",
            "<|startoftext|>Day 9 of #66DaysOfDataCompleted the Boolean and Conditional probability modules in the Data Structures and Algorithms courseAdded code to my in-built functions in my functions.hs book. PS: Now I can use all the theory that I learnt working with LSTM to my advantage.Book Link: https://t.co/z9aLt8YANN https://t.co/q3qV5qjsC<|endoftext|>\n",
            "<|startoftext|>Day 12 of #66daysofdata: Today, I finished the Boolean and Conditional probability modules in the Data Structures and Algorithms course on @cour\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vYZo7jNvRJFB"
      },
      "source": [
        "## Test generated tweets for similarity"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2MlWtTL1cRuq"
      },
      "source": [
        "import pandas as pd\n",
        "import numpy as np"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DK4pC_42RQ9d"
      },
      "source": [
        "tweet_data = pd.read_csv('/content/drive/MyDrive/66Days-Generator/finalData.csv')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 419
        },
        "id": "wTNuUqhARq_E",
        "outputId": "9c988dc7-0478-4be5-e970-98acaf31ab3f"
      },
      "source": [
        "tweet_data"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>full_text</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>@KenJee_DS looking forward to #66DaysOfData</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>Very excited to announce the #66daysofdata ini...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>@KenJee_DS Looking fwd to #66Daysofdata</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>Let’s start #66daysofdata https://t.co/IPm1WhHaHB</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>@KenJee_DS count me in #66daysofdata</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>14743</th>\n",
              "      <td>R2: #66daysofdata with @KenJee_DS Day 27: Had ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>14744</th>\n",
              "      <td>Days 16-18(R2) of #66daysofdata by @KenJee_DSF...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>14745</th>\n",
              "      <td>Day 40 of  #66DaysOfData r2:read the first sec...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>14746</th>\n",
              "      <td>Day 4 of #100DaysOfCode ; #66daysofdata    ✔️D...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>14747</th>\n",
              "      <td>Day 14 of #66DaysOfData:- Skillfactory course-...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>14748 rows × 1 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "                                               full_text\n",
              "0            @KenJee_DS looking forward to #66DaysOfData\n",
              "1      Very excited to announce the #66daysofdata ini...\n",
              "2                @KenJee_DS Looking fwd to #66Daysofdata\n",
              "3      Let’s start #66daysofdata https://t.co/IPm1WhHaHB\n",
              "4                   @KenJee_DS count me in #66daysofdata\n",
              "...                                                  ...\n",
              "14743  R2: #66daysofdata with @KenJee_DS Day 27: Had ...\n",
              "14744  Days 16-18(R2) of #66daysofdata by @KenJee_DSF...\n",
              "14745  Day 40 of  #66DaysOfData r2:read the first sec...\n",
              "14746  Day 4 of #100DaysOfCode ; #66daysofdata    ✔️D...\n",
              "14747  Day 14 of #66DaysOfData:- Skillfactory course-...\n",
              "\n",
              "[14748 rows x 1 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YTgxM6RqSmyB"
      },
      "source": [
        "### Check similarity the naive way\n",
        "\n",
        "- simply check if a generated tweet is exactly the same as in the data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YZ6FkrEeRst6",
        "outputId": "5bbc43aa-5703-4236-80d7-7d86ff31fad4"
      },
      "source": [
        "str_ = \"\"\"Day 3 of #66daysofdata. I used the time to brush up on some statistics.I learnt about the distributions and different means of doing the EDA. I also interacted with some datasets and played around with them.\"\"\"\n",
        "\n",
        "if tweet_data['full_text'].str.contains(str_).any():\n",
        "  print('yes')\n",
        "else: print('no')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "no\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3MNok22IVOSU"
      },
      "source": [
        "## Use Spacy-NLP to describe similarity"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eGIhMViWTR4Q",
        "outputId": "efb43b89-3737-4e7b-c728-7ba3b9f6b531"
      },
      "source": [
        "# get the nlp model\n",
        "!python -m spacy download en_core_web_md"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting en_core_web_md==2.2.5\n",
            "\u001b[?25l  Downloading https://github.com/explosion/spacy-models/releases/download/en_core_web_md-2.2.5/en_core_web_md-2.2.5.tar.gz (96.4MB)\n",
            "\u001b[K     |████████████████████████████████| 96.4MB 1.1MB/s \n",
            "\u001b[?25hRequirement already satisfied: spacy>=2.2.2 in /usr/local/lib/python3.7/dist-packages (from en_core_web_md==2.2.5) (2.2.4)\n",
            "Requirement already satisfied: srsly<1.1.0,>=1.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_md==2.2.5) (1.0.5)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_md==2.2.5) (4.41.1)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_md==2.2.5) (1.0.5)\n",
            "Requirement already satisfied: thinc==7.4.0 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_md==2.2.5) (7.4.0)\n",
            "Requirement already satisfied: numpy>=1.15.0 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_md==2.2.5) (1.19.5)\n",
            "Requirement already satisfied: catalogue<1.1.0,>=0.0.7 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_md==2.2.5) (1.0.0)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_md==2.2.5) (2.0.5)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_md==2.2.5) (2.23.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_md==2.2.5) (56.0.0)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_md==2.2.5) (3.0.5)\n",
            "Requirement already satisfied: blis<0.5.0,>=0.4.0 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_md==2.2.5) (0.4.1)\n",
            "Requirement already satisfied: plac<1.2.0,>=0.9.6 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_md==2.2.5) (1.1.3)\n",
            "Requirement already satisfied: wasabi<1.1.0,>=0.4.0 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_md==2.2.5) (0.8.2)\n",
            "Requirement already satisfied: importlib-metadata>=0.20; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from catalogue<1.1.0,>=0.0.7->spacy>=2.2.2->en_core_web_md==2.2.5) (3.10.1)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.2->en_core_web_md==2.2.5) (2.10)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.2->en_core_web_md==2.2.5) (1.24.3)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.2->en_core_web_md==2.2.5) (3.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.2->en_core_web_md==2.2.5) (2020.12.5)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=0.20; python_version < \"3.8\"->catalogue<1.1.0,>=0.0.7->spacy>=2.2.2->en_core_web_md==2.2.5) (3.4.1)\n",
            "Requirement already satisfied: typing-extensions>=3.6.4; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=0.20; python_version < \"3.8\"->catalogue<1.1.0,>=0.0.7->spacy>=2.2.2->en_core_web_md==2.2.5) (3.7.4.3)\n",
            "Building wheels for collected packages: en-core-web-md\n",
            "  Building wheel for en-core-web-md (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for en-core-web-md: filename=en_core_web_md-2.2.5-cp37-none-any.whl size=98051305 sha256=960b2e6d2f20da99df0596325e3174e228160257d9d1e7bc80f07c64bb563a7c\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-4tysq1l8/wheels/df/94/ad/f5cf59224cea6b5686ac4fd1ad19c8a07bc026e13c36502d81\n",
            "Successfully built en-core-web-md\n",
            "Installing collected packages: en-core-web-md\n",
            "Successfully installed en-core-web-md-2.2.5\n",
            "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
            "You can now load the model via spacy.load('en_core_web_md')\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Onpu0Ns4Vl_c"
      },
      "source": [
        "import spacy\n",
        "# load the language model\n",
        "nlp = spacy.load(\"en_core_web_md\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Sd1O5x31V_Tp",
        "outputId": "b20fafb8-2039-4253-c03d-b2256c5719c5"
      },
      "source": [
        "# check similarity\n",
        "doc1 = nlp(\"\"\"Day 3 of #66daysofdata. I used the time to brush up on some statistics.I learnt about the distributions and different means of doing the EDA. I also interacted with some datasets and played around with them.\"\"\")\n",
        "doc2 = nlp(\"\"\"How do I turn tv on/of?\"\"\")\n",
        "doc1.similarity(doc2)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.8649768063352461"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FasQLPp9W1gi"
      },
      "source": [
        "# loop over the real tweets\n",
        "# returns a list with the similarity to each tweet\n",
        "similarity_scores = []\n",
        "doc1 = nlp(\"\"\"Day 3 of #66daysofdata. I used the time to brush up on some statistics.I learnt about the distributions and different means of doing the EDA. I also interacted with some datasets and played around with them.\"\"\")\n",
        "\n",
        "for x in range(0,1000):\n",
        "  score_ = doc1.similarity(nlp(tweet_data.full_text[x]))    \n",
        "  similarity_scores.append(score_)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "L6-TrYeXYYnN",
        "outputId": "475538e1-7f31-4524-9be3-01c6c0c7c926"
      },
      "source": [
        "# Average similarity\n",
        "# higher better so generated tweet is similar to real tweet\n",
        "sum(similarity_scores) / len(similarity_scores)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.8709303380014755"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 29
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XcL2rGtpZLkk",
        "outputId": "5bf42023-caf1-422b-ec4f-0b9ef085d924"
      },
      "source": [
        "max(similarity_scores)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.9673041213495218"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 30
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dOyPqeymaom6",
        "outputId": "e940204d-6ddc-4b0a-adcb-339d9e60e5b6"
      },
      "source": [
        "# https://stackoverflow.com/questions/2474015/getting-the-index-of-the-returned-max-or-min-item-using-max-min-on-a-list\n",
        "max(range(len(similarity_scores)), key=similarity_scores.__getitem__)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "646"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 32
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "R1DwkwYobvP8",
        "outputId": "93f05065-902e-468a-aab6-ab069ec0272f"
      },
      "source": [
        "min(range(len(similarity_scores)), key=similarity_scores.__getitem__)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "789"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 37
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "j_O71REma51p",
        "outputId": "90373b5f-44fd-47e0-9e30-36a520d03c08"
      },
      "source": [
        "similarity_scores[646]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.9673041213495218"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 33
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 71
        },
        "id": "QkRCRqMrbWxp",
        "outputId": "f8d756eb-e005-40d5-ff7e-f237d157188a"
      },
      "source": [
        "tweet_data.full_text[646]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "\"Day 6 of #66daysofdataToday I performed Exploratory Data Analysis on Telco's Churn dataset. I really like that domain. I have seen other's kaggle kernels, and I realized that I have to learn doing some advanced visualizations. I also make note of the packages used. @KenJee_DS\""
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 36
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        },
        "id": "FRM3Fr2gbZL2",
        "outputId": "6660146e-185c-4ffa-cabe-af12e032ac82"
      },
      "source": [
        "tweet_data.full_text[789]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'#Dia5 de #66daysofdata por @KenJee_DSMiren esta belleza!!Primer capitulo realizado ✅ Mi primera aplicacion entendida e implementada en #pythonprogramming usando #JupyterNotebooks 👩🏻\\u200d💻🥳 https://t.co/WIBcZ45Isa'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 38
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1JHoBwQSbzwF",
        "outputId": "14fd8e60-bab1-4d33-dbd4-96bd2bb6e28b"
      },
      "source": [
        "min(similarity_scores)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.09145792753401237"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 39
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IQIXYbatcCfT",
        "outputId": "f2df78be-fc69-44f1-cb30-44e70a99040f"
      },
      "source": [
        "similarity_scores[789]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.09145792753401237"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 40
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VJwZ2zs_c27c"
      },
      "source": [
        "Lets remove stop words since they tend to inflate the similarity score.\n",
        "\n",
        "https://stackoverflow.com/questions/52113939/spacy-strange-similarity-between-two-sentences"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UTQPj4u5cG5Q"
      },
      "source": [
        "# https://betterprogramming.pub/the-beginners-guide-to-similarity-matching-using-spacy-782fc2922f7c\n",
        "def remove_stopwords_fast(text):\n",
        "    doc = nlp(text.lower())\n",
        "    result = [token.text for token in doc if token.text not in nlp.Defaults.stop_words]\n",
        "    return \" \".join(result)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 37
        },
        "id": "XYiN4zYQdKDh",
        "outputId": "5ff17389-5f8c-42dd-e730-035da6b6855a"
      },
      "source": [
        "text = \"\"\"Day 3 of #66daysofdata. I used the time to brush up on some statistics.I learnt about the distributions and different means of doing the EDA. I also interacted with some datasets and played around with them.\"\"\"\n",
        "remove_stopwords_fast(text)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'day 3 # 66daysofdata . time brush statistics.i learnt distributions different means eda . interacted datasets played .'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 42
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "veLqVXdgdTV9"
      },
      "source": [
        "similarity_scores_stop = []\n",
        "doc1 = nlp(remove_stopwords_fast(text))\n",
        "\n",
        "for x in range(0,1000):\n",
        "  score_ = doc1.similarity(nlp(remove_stopwords_fast(tweet_data.full_text[x])))    \n",
        "  similarity_scores_stop.append(score_)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0xx_Ds3ZeOPP",
        "outputId": "77cf9c10-543b-4b90-9842-fc0ae50ad785"
      },
      "source": [
        "# removing stop words reduced the similarity of 10 percent points\n",
        "sum(similarity_scores_stop) / len(similarity_scores_stop)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.7756576540945388"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 45
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "29Tgf4IXelrB",
        "outputId": "cb7dd286-53d9-49e6-930a-80ab0f7b42f1"
      },
      "source": [
        "max(similarity_scores_stop)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.9211037812250895"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 47
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "p0PcXJJnew0c",
        "outputId": "38c2af50-0774-4983-c009-45ad936d9bd0"
      },
      "source": [
        "max(range(len(similarity_scores)), key=similarity_scores.__getitem__)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "646"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 48
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        },
        "id": "L3VOYCW3e14M",
        "outputId": "768d452e-048a-4b58-c949-a5d4cd2394ba"
      },
      "source": [
        "remove_stopwords_fast(tweet_data.full_text[646])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'day 6 # 66daysofdatatoday performed exploratory data analysis telco churn dataset . like domain . seen kaggle kernels , realized learn advanced visualizations . note packages . @kenjee_ds'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 49
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6gg3-8M5fqrv"
      },
      "source": [
        "### Limitaions for using Spacy-similarity scores\n",
        "> Computing similarity scores can be helpful in many situations, but it’s also important to maintain realistic expectations about what information it can provide. Words can be related to each other in many ways, so a single “similarity” score will always be a mix of different signals, and vectors trained on different data can produce very different results that may not be useful for your purpose (https://spacy.io/usage/linguistic-features#vectors-similarity)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XDhQkGiIg6LB"
      },
      "source": [
        "Google's Universal Sentence Encoder: https://tfhub.dev/google/universal-sentence-encoder/4. \n",
        "\n",
        "Facebook's Infersent Encoder: https://github.com/facebookresearch/InferSent"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dMjodx43mhoB"
      },
      "source": [
        "## Google's Universal Sentence Encoder\n",
        ">  encodes text into high-dimensional vectors that can be used for text classification, semantic similarity, clustering and other natural language tasks.\n",
        "\n",
        "- traind and optimaized for greater-then-word length text\n",
        "- INPUT: English text of variable length\n",
        "- OUTUPU: a 512 dimensional vector \n",
        "\n",
        "- no preprocessing is necessary (stop word removal)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ThOcLgkhe_Po",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "73f6862b-02ea-46b7-9e73-a05e011cb300"
      },
      "source": [
        "# load the the model\n",
        "from absl import logging\n",
        "\n",
        "import tensorflow as tf\n",
        "\n",
        "import tensorflow_hub as hub\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import os\n",
        "import pandas as pd\n",
        "import re\n",
        "import seaborn as sns\n",
        "\n",
        "module_url = \"https://tfhub.dev/google/universal-sentence-encoder/4\"\n",
        "model = hub.load(module_url)\n",
        "print (\"module %s loaded\" % module_url)\n",
        "def embed(input):\n",
        "  return model(input)"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "module https://tfhub.dev/google/universal-sentence-encoder/4 loaded\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OOE3MBrIom17"
      },
      "source": [
        "tweet_data = pd.read_csv('/content/drive/MyDrive/66Days-Generator/finalData.csv')"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xwlOaP6eoqIy"
      },
      "source": [
        "# choose tweet 646 since it had the hightest similrity score using spaCy\n",
        "real_tweet = tweet_data.full_text[646]\n",
        "# generated tweet\n",
        "generated_tweet = \"\"\"Day 3 of #66daysofdata. I used the time to brush up on some statistics.I learnt about the distributions and different means of doing the EDA. I also interacted with some datasets and played around with them.\"\"\"\n",
        "tweets = [real_tweet, generated_tweet]"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "S5pM9RDVnkXA"
      },
      "source": [
        "# Reduce logging outpur\n",
        "logging.set_verbosity(logging.ERROR)\n",
        "\n",
        "# compute the embeddigns\n",
        "message_embeddings = embed(tweets)"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eXun9-CfppIF",
        "outputId": "ddf3e09e-7a5c-4981-c3f1-8b25cc785486"
      },
      "source": [
        "message_embeddings"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tf.Tensor: shape=(2, 512), dtype=float32, numpy=\n",
              "array([[-0.01785527, -0.06353536,  0.01804276, ...,  0.08779907,\n",
              "        -0.05331371, -0.06075975],\n",
              "       [-0.03645666, -0.04723918, -0.03063959, ...,  0.09627496,\n",
              "        -0.03469933, -0.06400408]], dtype=float32)>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ST1tgUUYpqNb",
        "outputId": "4e1c735f-aed8-42e7-fb76-cd93e20d56b0"
      },
      "source": [
        "for i, message_embedding in enumerate(np.array(message_embeddings).tolist()):\n",
        "  print(\"Message: {}\".format(tweets[i]))\n",
        "  print(\"Embedding size: {}\".format(len(message_embedding)))\n",
        "  message_embedding_snippet = \", \".join(\n",
        "      (str(x) for x in message_embedding[:3]))\n",
        "  print(\"Embedding: [{}, ...]\\n\".format(message_embedding_snippet))"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Message: Day 6 of #66daysofdataToday I performed Exploratory Data Analysis on Telco's Churn dataset. I really like that domain. I have seen other's kaggle kernels, and I realized that I have to learn doing some advanced visualizations. I also make note of the packages used. @KenJee_DS\n",
            "Embedding size: 512\n",
            "Embedding: [-0.017855273559689522, -0.06353535503149033, 0.01804276369512081, ...]\n",
            "\n",
            "Message: Day 3 of #66daysofdata. I used the time to brush up on some statistics.I learnt about the distributions and different means of doing the EDA. I also interacted with some datasets and played around with them.\n",
            "Embedding size: 512\n",
            "Embedding: [-0.03645665571093559, -0.04723918065428734, -0.030639585107564926, ...]\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EB008ZRWpzmH",
        "outputId": "ad95e773-6f4c-4ffd-aead-8454fe34593e"
      },
      "source": [
        "# The semantic similarity of two sentences can be trivially computed as the inner product of the encodings\n",
        "np.inner(message_embeddings[0], message_embeddings[1])"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.5870047"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8mRqkhLpq6Fu"
      },
      "source": [
        "This is compared to the score form sypCy quite low, showing the stated limitiations of the approch from spaCy"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "q8qUFvQcqc3u"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}