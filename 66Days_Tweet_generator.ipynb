{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "66Days_Tweet-generator.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "AUc4vG-d-LzC"
      ]
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xysjTO68-Eb1"
      },
      "source": [
        "# Tweet-generator for #66DaysofData\n",
        "\n",
        "In this notebook I will train a <a href=\"https://openai.com/blog/better-language-models/\">GTP-2</a> model with collected tweets. I collected almost 15000 tweets form the #66DayofData-challenge on Twitter. You can read about the process on my <a href=\"https://markusmueller-ds.github.io/portfolio/66days_analysis.html\">website</a>. The goal is to create a tweet generator based on this dataset.\n",
        "\n",
        "The creator of the <a href=\"https://github.com/minimaxir/gpt-2-simple\">gtp-2-simple</a> libary publisched a great <a href=\"https://minimaxir.com/2020/01/twitter-gpt2-bot/\">blog post</a> explaining the process of using GTP-2 to create a tweet generator. \n",
        "\n",
        "### What is GPT-2?\n",
        "> GPT-2 is a large transformer-based language model with 1.5 billion parameters, trained on a dataset of 8 million web pages (40GB text data).\n",
        "\n",
        "### Trainings-parameters\n",
        "25.04.2021\n",
        "- model: '124M'\n",
        "- steps: 2000\n",
        "- run_name: 'run1'\n",
        "\n",
        "Training time: 01:18\n",
        "Evaluation: avg loss =1.20\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pWsV35WqN8y6"
      },
      "source": [
        "## Imports"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "h39jWc9tAmXB",
        "outputId": "c6c8fea6-d07a-42e6-ee75-37b6fd8acbe5"
      },
      "source": [
        "%tensorflow_version 1.x\n",
        "!pip install -q gpt-2-simple\n",
        "import gpt_2_simple as gpt2\n",
        "from datetime import datetime"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "TensorFlow 1.x selected.\n",
            "WARNING:tensorflow:\n",
            "The TensorFlow contrib module will not be included in TensorFlow 2.0.\n",
            "For more information, please see:\n",
            "  * https://github.com/tensorflow/community/blob/master/rfcs/20180907-contrib-sunset.md\n",
            "  * https://github.com/tensorflow/addons\n",
            "  * https://github.com/tensorflow/io (for I/O related ops)\n",
            "If you depend on functionality not listed there, please file an issue.\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MeajDRFCEOEE",
        "outputId": "4425ba03-a0d2-48d1-eb5e-f53dff644ffe"
      },
      "source": [
        "gpt2.mount_gdrive()"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LuNZUOqWBW76",
        "outputId": "58c80d74-9f54-49b6-b2e0-b5790af09ddc"
      },
      "source": [
        "# check gpu\n",
        "# best case: Nvidia P100 GPU\n",
        "!nvidia-smi"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Sun Apr 25 08:24:43 2021       \n",
            "+-----------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 465.19.01    Driver Version: 460.32.03    CUDA Version: 11.2     |\n",
            "|-------------------------------+----------------------+----------------------+\n",
            "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
            "|                               |                      |               MIG M. |\n",
            "|===============================+======================+======================|\n",
            "|   0  Tesla T4            Off  | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   63C    P8    11W /  70W |      0MiB / 15109MiB |      0%      Default |\n",
            "|                               |                      |                  N/A |\n",
            "+-------------------------------+----------------------+----------------------+\n",
            "                                                                               \n",
            "+-----------------------------------------------------------------------------+\n",
            "| Processes:                                                                  |\n",
            "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
            "|        ID   ID                                                   Usage      |\n",
            "|=============================================================================|\n",
            "|  No running processes found                                                 |\n",
            "+-----------------------------------------------------------------------------+\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AUc4vG-d-LzC"
      },
      "source": [
        "## Load and prepare data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oGarc26A95pS"
      },
      "source": [
        "import numpy as np\n",
        "import pandas as pd"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uUBbFmKV-XK7"
      },
      "source": [
        "data = pd.read_csv('/content/drive/MyDrive/66Days-Generator/finalFrame.csv')"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 711
        },
        "id": "qqJxSfvR-f4Q",
        "outputId": "a2b29d3e-771f-42e8-ebbc-71b24c5d3997"
      },
      "source": [
        "data"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>tweet_id</th>\n",
              "      <th>user_id</th>\n",
              "      <th>user_name</th>\n",
              "      <th>created_at</th>\n",
              "      <th>full_text</th>\n",
              "      <th>retweets</th>\n",
              "      <th>favorite</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>1299601482749181952</td>\n",
              "      <td>1292469347370360839</td>\n",
              "      <td>DuckPython</td>\n",
              "      <td>2020-08-29 06:55:13+00:00</td>\n",
              "      <td>@KenJee_DS looking forward to #66DaysOfData</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1299734773456203777</td>\n",
              "      <td>1159830350102781953</td>\n",
              "      <td>KenJee_DS</td>\n",
              "      <td>2020-08-29 15:44:52+00:00</td>\n",
              "      <td>Very excited to announce the #66daysofdata ini...</td>\n",
              "      <td>51</td>\n",
              "      <td>269</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>1299735515923505153</td>\n",
              "      <td>719854244</td>\n",
              "      <td>Sachin_g_here</td>\n",
              "      <td>2020-08-29 15:47:49+00:00</td>\n",
              "      <td>@KenJee_DS Looking fwd to #66Daysofdata</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>1299735809004769282</td>\n",
              "      <td>1001046433695285249</td>\n",
              "      <td>gautham53814486</td>\n",
              "      <td>2020-08-29 15:48:59+00:00</td>\n",
              "      <td>Let‚Äôs start #66daysofdata https://t.co/IPm1WhHaHB</td>\n",
              "      <td>0</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>1299736210575769605</td>\n",
              "      <td>1652520728</td>\n",
              "      <td>khudiamayankino</td>\n",
              "      <td>2020-08-29 15:50:35+00:00</td>\n",
              "      <td>@KenJee_DS count me in #66daysofdata</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>17241</th>\n",
              "      <td>1381306986067750918</td>\n",
              "      <td>731856877139558400</td>\n",
              "      <td>ABYA80</td>\n",
              "      <td>2021-04-11 18:03:43+00:00</td>\n",
              "      <td>R2: #66daysofdata with @KenJee_DS \\n\\nDay 27: ...</td>\n",
              "      <td>1</td>\n",
              "      <td>14</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>17242</th>\n",
              "      <td>1381326847527550983</td>\n",
              "      <td>324583975</td>\n",
              "      <td>georgekanellos</td>\n",
              "      <td>2021-04-11 19:22:38+00:00</td>\n",
              "      <td>Days 16-18(R2) of #66daysofdata by @KenJee_DS\\...</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>17243</th>\n",
              "      <td>1381336589641646083</td>\n",
              "      <td>1300492664308146176</td>\n",
              "      <td>MarkusM99098101</td>\n",
              "      <td>2021-04-11 20:01:21+00:00</td>\n",
              "      <td>Day 40 of  #66DaysOfData r2:\\n\\nread the first...</td>\n",
              "      <td>0</td>\n",
              "      <td>5</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>17244</th>\n",
              "      <td>1381338886241157124</td>\n",
              "      <td>1282311789464760321</td>\n",
              "      <td>HeqiqetEhmedova</td>\n",
              "      <td>2021-04-11 20:10:29+00:00</td>\n",
              "      <td>Day 4 of #100DaysOfCode ; #66daysofdata \\n   ‚úî...</td>\n",
              "      <td>10</td>\n",
              "      <td>5</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>17245</th>\n",
              "      <td>1381343032751046658</td>\n",
              "      <td>32453704</td>\n",
              "      <td>anyachocolat</td>\n",
              "      <td>2021-04-11 20:26:57+00:00</td>\n",
              "      <td>Day 14 of #66DaysOfData:\\n- Skillfactory cours...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>17246 rows √ó 7 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "                  tweet_id              user_id  ... retweets favorite\n",
              "0      1299601482749181952  1292469347370360839  ...        0        1\n",
              "1      1299734773456203777  1159830350102781953  ...       51      269\n",
              "2      1299735515923505153            719854244  ...        0        1\n",
              "3      1299735809004769282  1001046433695285249  ...        0        2\n",
              "4      1299736210575769605           1652520728  ...        0        1\n",
              "...                    ...                  ...  ...      ...      ...\n",
              "17241  1381306986067750918   731856877139558400  ...        1       14\n",
              "17242  1381326847527550983            324583975  ...        0        1\n",
              "17243  1381336589641646083  1300492664308146176  ...        0        5\n",
              "17244  1381338886241157124  1282311789464760321  ...       10        5\n",
              "17245  1381343032751046658             32453704  ...        0        0\n",
              "\n",
              "[17246 rows x 7 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QG3OwzFu-hlk",
        "outputId": "cae26911-2be8-49af-d62a-f2e24aa2e857"
      },
      "source": [
        "data.duplicated('tweet_id').sum()"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "2500"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qvHdv2aS-t5e"
      },
      "source": [
        "# drop duplicates\n",
        "data = data.drop_duplicates(subset=['tweet_id'], keep='last')"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IcMOqz8X-0eH"
      },
      "source": [
        "# frop unrelevant columns\n",
        "data = data[['full_text']]"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QMx4pAPu_NNF"
      },
      "source": [
        "data.reset_index(inplace=True, drop=True)"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 419
        },
        "id": "fR-AfUfx_dZP",
        "outputId": "48221af0-6723-4b75-8805-6a3ea99766a2"
      },
      "source": [
        "data"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>full_text</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>@KenJee_DS looking forward to #66DaysOfData</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>Very excited to announce the #66daysofdata ini...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>@KenJee_DS Looking fwd to #66Daysofdata</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>Let‚Äôs start #66daysofdata https://t.co/IPm1WhHaHB</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>@KenJee_DS count me in #66daysofdata</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>14741</th>\n",
              "      <td>R2: #66daysofdata with @KenJee_DS \\n\\nDay 27: ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>14742</th>\n",
              "      <td>Days 16-18(R2) of #66daysofdata by @KenJee_DS\\...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>14743</th>\n",
              "      <td>Day 40 of  #66DaysOfData r2:\\n\\nread the first...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>14744</th>\n",
              "      <td>Day 4 of #100DaysOfCode ; #66daysofdata \\n   ‚úî...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>14745</th>\n",
              "      <td>Day 14 of #66DaysOfData:\\n- Skillfactory cours...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>14746 rows √ó 1 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "                                               full_text\n",
              "0            @KenJee_DS looking forward to #66DaysOfData\n",
              "1      Very excited to announce the #66daysofdata ini...\n",
              "2                @KenJee_DS Looking fwd to #66Daysofdata\n",
              "3      Let‚Äôs start #66daysofdata https://t.co/IPm1WhHaHB\n",
              "4                   @KenJee_DS count me in #66daysofdata\n",
              "...                                                  ...\n",
              "14741  R2: #66daysofdata with @KenJee_DS \\n\\nDay 27: ...\n",
              "14742  Days 16-18(R2) of #66daysofdata by @KenJee_DS\\...\n",
              "14743  Day 40 of  #66DaysOfData r2:\\n\\nread the first...\n",
              "14744  Day 4 of #100DaysOfCode ; #66daysofdata \\n   ‚úî...\n",
              "14745  Day 14 of #66DaysOfData:\\n- Skillfactory cours...\n",
              "\n",
              "[14746 rows x 1 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XVgLKk4Y_kvp"
      },
      "source": [
        "# remove new line char\n",
        "data.full_text = data.full_text.replace(r'\\n','', regex=True)"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 419
        },
        "id": "otq7sSJZ_5E4",
        "outputId": "41a6433b-e680-44e1-d2ba-ce4a1367ee66"
      },
      "source": [
        "data"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>full_text</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>@KenJee_DS looking forward to #66DaysOfData</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>Very excited to announce the #66daysofdata ini...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>@KenJee_DS Looking fwd to #66Daysofdata</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>Let‚Äôs start #66daysofdata https://t.co/IPm1WhHaHB</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>@KenJee_DS count me in #66daysofdata</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>14741</th>\n",
              "      <td>R2: #66daysofdata with @KenJee_DS Day 27: Had ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>14742</th>\n",
              "      <td>Days 16-18(R2) of #66daysofdata by @KenJee_DSF...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>14743</th>\n",
              "      <td>Day 40 of  #66DaysOfData r2:read the first sec...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>14744</th>\n",
              "      <td>Day 4 of #100DaysOfCode ; #66daysofdata    ‚úîÔ∏èD...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>14745</th>\n",
              "      <td>Day 14 of #66DaysOfData:- Skillfactory course-...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>14746 rows √ó 1 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "                                               full_text\n",
              "0            @KenJee_DS looking forward to #66DaysOfData\n",
              "1      Very excited to announce the #66daysofdata ini...\n",
              "2                @KenJee_DS Looking fwd to #66Daysofdata\n",
              "3      Let‚Äôs start #66daysofdata https://t.co/IPm1WhHaHB\n",
              "4                   @KenJee_DS count me in #66daysofdata\n",
              "...                                                  ...\n",
              "14741  R2: #66daysofdata with @KenJee_DS Day 27: Had ...\n",
              "14742  Days 16-18(R2) of #66daysofdata by @KenJee_DSF...\n",
              "14743  Day 40 of  #66DaysOfData r2:read the first sec...\n",
              "14744  Day 4 of #100DaysOfCode ; #66daysofdata    ‚úîÔ∏èD...\n",
              "14745  Day 14 of #66DaysOfData:- Skillfactory course-...\n",
              "\n",
              "[14746 rows x 1 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WI5POtxoClGX"
      },
      "source": [
        "# save file\n",
        "data.to_csv('finalData.csv', index=False)"
      ],
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TPDnBs-2CsYl"
      },
      "source": [
        "final_data = pd.read_csv('/content/finalData.csv')"
      ],
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 419
        },
        "id": "y2o_lMGAC1xo",
        "outputId": "1dfd1bc0-6732-4231-963b-031945916b35"
      },
      "source": [
        "final_data"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>full_text</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>@KenJee_DS looking forward to #66DaysOfData</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>Very excited to announce the #66daysofdata ini...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>@KenJee_DS Looking fwd to #66Daysofdata</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>Let‚Äôs start #66daysofdata https://t.co/IPm1WhHaHB</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>@KenJee_DS count me in #66daysofdata</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>14743</th>\n",
              "      <td>R2: #66daysofdata with @KenJee_DS Day 27: Had ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>14744</th>\n",
              "      <td>Days 16-18(R2) of #66daysofdata by @KenJee_DSF...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>14745</th>\n",
              "      <td>Day 40 of  #66DaysOfData r2:read the first sec...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>14746</th>\n",
              "      <td>Day 4 of #100DaysOfCode ; #66daysofdata    ‚úîÔ∏èD...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>14747</th>\n",
              "      <td>Day 14 of #66DaysOfData:- Skillfactory course-...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>14748 rows √ó 1 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "                                               full_text\n",
              "0            @KenJee_DS looking forward to #66DaysOfData\n",
              "1      Very excited to announce the #66daysofdata ini...\n",
              "2                @KenJee_DS Looking fwd to #66Daysofdata\n",
              "3      Let‚Äôs start #66daysofdata https://t.co/IPm1WhHaHB\n",
              "4                   @KenJee_DS count me in #66daysofdata\n",
              "...                                                  ...\n",
              "14743  R2: #66daysofdata with @KenJee_DS Day 27: Had ...\n",
              "14744  Days 16-18(R2) of #66daysofdata by @KenJee_DSF...\n",
              "14745  Day 40 of  #66DaysOfData r2:read the first sec...\n",
              "14746  Day 4 of #100DaysOfCode ; #66daysofdata    ‚úîÔ∏èD...\n",
              "14747  Day 14 of #66DaysOfData:- Skillfactory course-...\n",
              "\n",
              "[14748 rows x 1 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UZhnEmtYGrn2"
      },
      "source": [
        "gpt2.encode_csv('/content/drive/MyDrive/66Days-Generator/finalData.csv')"
      ],
      "execution_count": 36,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6_Y7r-znCChy"
      },
      "source": [
        "## Train GPT-2\n",
        "I used the 124M base model. There are more performant modeks but hty use more disk space and are more suitable for longer texts, which is not the case for me.\n",
        "- Other models: 355M, 774M and 1558M"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "t4JtkrItAGKO",
        "outputId": "04880abb-3caf-488a-80bb-d353e22ddbec"
      },
      "source": [
        "# downlad model\n",
        "gpt2.download_gpt2(model_name=\"124M\")"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Fetching checkpoint: 1.05Mit [00:00, 242Mit/s]                                                      \n",
            "Fetching encoder.json: 1.05Mit [00:00, 4.91Mit/s]\n",
            "Fetching hparams.json: 1.05Mit [00:00, 507Mit/s]                                                    \n",
            "Fetching model.ckpt.data-00000-of-00001: 498Mit [00:11, 42.2Mit/s]                                  \n",
            "Fetching model.ckpt.index: 1.05Mit [00:00, 334Mit/s]                                                \n",
            "Fetching model.ckpt.meta: 1.05Mit [00:00, 7.97Mit/s]\n",
            "Fetching vocab.bpe: 1.05Mit [00:00, 6.05Mit/s]\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y6xhhDYgP317"
      },
      "source": [
        "The following code will finetune the GPT-2 model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9p5Ujx14DDrl",
        "outputId": "48cc586f-0acc-4330-e65b-147694003a0a"
      },
      "source": [
        "sess = gpt2.start_tf_sess()\n",
        "\n",
        "gpt2.finetune(sess,\n",
        "              dataset='/content/drive/MyDrive/66Days-Generator/csv_encoded.txt',\n",
        "              model_name='124M',\n",
        "              steps=2000,\n",
        "              restore_from='fresh',\n",
        "              run_name='run1',\n",
        "              print_every=10,\n",
        "              sample_every=500,\n",
        "              save_every=500\n",
        "              )"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.7/dist-packages/gpt_2_simple/src/sample.py:17: where (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
            "Loading checkpoint models/124M/model.ckpt\n",
            "INFO:tensorflow:Restoring parameters from models/124M/model.ckpt\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\r  0%|          | 0/1 [00:00<?, ?it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Loading dataset...\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:04<00:00,  4.61s/it]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "dataset has 979860 tokens\n",
            "Training...\n",
            "[10 | 29.27] loss=2.89 avg=2.89\n",
            "[20 | 52.25] loss=2.78 avg=2.83\n",
            "[30 | 75.78] loss=2.62 avg=2.76\n",
            "[40 | 98.88] loss=2.63 avg=2.73\n",
            "[50 | 121.79] loss=2.56 avg=2.69\n",
            "[60 | 144.93] loss=2.63 avg=2.68\n",
            "[70 | 168.11] loss=2.44 avg=2.65\n",
            "[80 | 191.19] loss=2.51 avg=2.63\n",
            "[90 | 214.29] loss=2.37 avg=2.60\n",
            "[100 | 237.45] loss=2.44 avg=2.58\n",
            "[110 | 260.60] loss=2.47 avg=2.57\n",
            "[120 | 283.76] loss=2.33 avg=2.55\n",
            "[130 | 306.92] loss=2.41 avg=2.54\n",
            "[140 | 330.08] loss=2.44 avg=2.53\n",
            "[150 | 353.21] loss=2.28 avg=2.51\n",
            "[160 | 376.35] loss=2.42 avg=2.51\n",
            "[170 | 399.48] loss=2.42 avg=2.50\n",
            "[180 | 422.61] loss=2.35 avg=2.49\n",
            "[190 | 445.73] loss=2.30 avg=2.48\n",
            "[200 | 468.84] loss=2.43 avg=2.48\n",
            "[210 | 491.96] loss=2.39 avg=2.47\n",
            "[220 | 515.08] loss=2.19 avg=2.46\n",
            "[230 | 538.22] loss=2.21 avg=2.45\n",
            "[240 | 561.36] loss=2.37 avg=2.44\n",
            "[250 | 584.49] loss=2.20 avg=2.43\n",
            "[260 | 607.64] loss=2.17 avg=2.42\n",
            "[270 | 630.74] loss=2.46 avg=2.42\n",
            "[280 | 653.83] loss=2.08 avg=2.41\n",
            "[290 | 676.92] loss=2.14 avg=2.40\n",
            "[300 | 700.03] loss=2.35 avg=2.40\n",
            "[310 | 723.12] loss=2.23 avg=2.39\n",
            "[320 | 746.23] loss=2.12 avg=2.38\n",
            "[330 | 769.38] loss=2.07 avg=2.37\n",
            "[340 | 792.55] loss=2.23 avg=2.37\n",
            "[350 | 815.70] loss=2.21 avg=2.36\n",
            "[360 | 838.83] loss=2.11 avg=2.35\n",
            "[370 | 861.97] loss=2.11 avg=2.34\n",
            "[380 | 885.11] loss=2.15 avg=2.34\n",
            "[390 | 908.24] loss=2.22 avg=2.33\n",
            "[400 | 931.32] loss=2.10 avg=2.33\n",
            "[410 | 954.45] loss=2.16 avg=2.32\n",
            "[420 | 977.61] loss=2.00 avg=2.31\n",
            "[430 | 1000.76] loss=2.07 avg=2.31\n",
            "[440 | 1023.92] loss=1.87 avg=2.29\n",
            "[450 | 1047.06] loss=2.05 avg=2.29\n",
            "[460 | 1070.15] loss=1.93 avg=2.28\n",
            "[470 | 1093.24] loss=2.11 avg=2.27\n",
            "[480 | 1116.36] loss=1.92 avg=2.26\n",
            "[490 | 1139.50] loss=1.86 avg=2.25\n",
            "[500 | 1162.63] loss=1.85 avg=2.24\n",
            "Saving checkpoint/run1/model-500\n",
            "======== SAMPLE 1 ========\n",
            ".\n",
            "Day 23: https://t.co/fL2R8NvWVnI The Ultimate #Titanic #Database #DataScience #Import Data#66daysofdata #DataAnalytics #DeepLearning #pythonprogramming #DataScience #monetization #AI #coding #programmingtwitter #Yahoo‚Äôs API<|endoftext|>\n",
            "<|startoftext|>Day 25 of #66DaysofData - Finished the Housing Prices Competition üå©Ô∏è<|endoftext|>\n",
            "<|startoftext|>Day 24 of #66daysofdata !- I went through the 3rd chapter of \"Intermediate Machine Learning\", by Aur√©lien G√©ron, a must for anyone new to Deep Learning !- I reviewed my book - \"Browsing the Brain - Hands-on Machine Learning\" @KenJee_DS<|endoftext|>\n",
            "<|startoftext|>Day 25 of #66daysofdata by Ken attended a conference on Data Engineering https://t.co/4V5NgVQXt9<|endoftext|>\n",
            "<|startoftext|>Day 15 of #66daysofdata Today I will continue with the 'Importing Data into MySQL' project by @datacamp. I have to admit it was a challenge but I was able to write scripts for the sake of the challenge...<|endoftext|>\n",
            "<|startoftext|>Day 25 of #66DaysOfDatacompleted some exercises on Python on using 'py4e' to display a chart. #DataCulture https://t.co/N8X0nEtXJg<|endoftext|>\n",
            "<|startoftext|>D.9 #66daysofdata Read the first chapter of \"The hundred-page machine learning book\". I've been watching videos on deep learning and how DBM can be used in Artificial Neural Networks.https://t.co/NlP1b3hRzq<|endoftext|>\n",
            "<|startoftext|>Day 7 of #66DaysofData! Today I‚Äôm going to use R for an interview. A great guide for Data Science Interviews. I just finished my first few videos. https://t.co/jIhBHQzYW6 #R<|endoftext|>\n",
            "<|startoftext|>Day 25 of #66daysofdata with @KenJee_DS Today I attended @YouTube's \"Introduction to Machine Learning with Scratch\" @KenJee_DS I have a hard time putting the same skills into some basic stuff... I know I can take more from it than I do from the actual skill level but I know I am missing the big picture.<|endoftext|>\n",
            "<|startoftext|>Day 27 of #Round2 #66daysofdata  with @KenJee_DS  Continued to explore the concepts of SQL, learned more about using the CASE WHEN operator on strings and how to use CASE WHEN.<|endoftext|>\n",
            "<|startoftext|>Day 26 of #66DaysOfData! Today I will continue to learn some SQL from Tableau.I started reviewing all the concepts of databases using JOINS on @Kaggle. https://t.co/5F5DgKtLW1g https://t.co/Vr6d6T0jK4<|endoftext|>\n",
            "<|startoftext|>Day 22: #66daysofdata - Did an exploratory data analysis project on SQLite - Studied about Tableau fundamentals in Python - Learned about SQL and SQLITE - Learnt how to create tables and how to insert and delete data - Learned about GROUP BY operator and JOINS!<|endoftext|>\n",
            "<|startoftext|>D25. #66DaysofData - Started SQL on Dataquest  on April 1st.Finished working on the EDA project from today.Read about the basic and advanced SQL syntax. #DataScience #Analytics<|endoftext|>\n",
            "<|startoftext|>Day 23 of #66daysofdataI have revised the basics of #NeuralNetworks (the basics are very similar to #NLP and its derivatives) by practicing some #DeepLearning algorithms (and in general applying them).https://t.co/p0cIqXgPtF<|endoftext|>\n",
            "<|startoftext|>#Day1 of #66DaysOfData with @KenJee_DS Today I am reading Chapter 8: Creating Neural Network. https://t.co/3t7Kgw\n",
            "\n",
            "[510 | 1198.77] loss=2.04 avg=2.24\n",
            "[520 | 1221.90] loss=2.03 avg=2.23\n",
            "[530 | 1244.96] loss=1.93 avg=2.23\n",
            "[540 | 1268.08] loss=2.05 avg=2.22\n",
            "[550 | 1291.20] loss=2.03 avg=2.22\n",
            "[560 | 1314.35] loss=1.82 avg=2.21\n",
            "[570 | 1337.50] loss=2.04 avg=2.20\n",
            "[580 | 1360.64] loss=1.75 avg=2.19\n",
            "[590 | 1383.77] loss=1.97 avg=2.19\n",
            "[600 | 1406.89] loss=1.80 avg=2.18\n",
            "[610 | 1430.00] loss=2.07 avg=2.18\n",
            "[620 | 1453.13] loss=1.97 avg=2.17\n",
            "[630 | 1476.23] loss=1.85 avg=2.17\n",
            "[640 | 1499.30] loss=1.86 avg=2.16\n",
            "[650 | 1522.37] loss=1.92 avg=2.16\n",
            "[660 | 1545.48] loss=1.53 avg=2.14\n",
            "[670 | 1568.59] loss=1.78 avg=2.13\n",
            "[680 | 1591.73] loss=1.72 avg=2.13\n",
            "[690 | 1614.85] loss=2.06 avg=2.13\n",
            "[700 | 1637.97] loss=1.75 avg=2.12\n",
            "[710 | 1661.09] loss=1.83 avg=2.11\n",
            "[720 | 1684.23] loss=1.88 avg=2.11\n",
            "[730 | 1707.35] loss=1.62 avg=2.10\n",
            "[740 | 1730.50] loss=1.77 avg=2.09\n",
            "[750 | 1753.64] loss=1.54 avg=2.08\n",
            "[760 | 1776.75] loss=1.84 avg=2.08\n",
            "[770 | 1799.88] loss=1.57 avg=2.07\n",
            "[780 | 1823.02] loss=1.53 avg=2.06\n",
            "[790 | 1846.13] loss=1.61 avg=2.05\n",
            "[800 | 1869.26] loss=1.51 avg=2.04\n",
            "[810 | 1892.38] loss=1.85 avg=2.04\n",
            "[820 | 1915.48] loss=1.48 avg=2.03\n",
            "[830 | 1938.58] loss=1.67 avg=2.02\n",
            "[840 | 1961.70] loss=1.50 avg=2.01\n",
            "[850 | 1984.80] loss=1.83 avg=2.01\n",
            "[860 | 2007.90] loss=1.77 avg=2.00\n",
            "[870 | 2031.00] loss=1.70 avg=2.00\n",
            "[880 | 2054.10] loss=1.71 avg=1.99\n",
            "[890 | 2077.19] loss=1.42 avg=1.98\n",
            "[900 | 2100.28] loss=1.84 avg=1.98\n",
            "[910 | 2123.37] loss=1.64 avg=1.98\n",
            "[920 | 2146.45] loss=1.47 avg=1.97\n",
            "[930 | 2169.54] loss=1.58 avg=1.96\n",
            "[940 | 2192.63] loss=1.43 avg=1.95\n",
            "[950 | 2215.70] loss=1.78 avg=1.95\n",
            "[960 | 2238.78] loss=1.41 avg=1.94\n",
            "[970 | 2261.85] loss=1.40 avg=1.93\n",
            "[980 | 2284.92] loss=1.31 avg=1.92\n",
            "[990 | 2307.98] loss=1.53 avg=1.92\n",
            "[1000 | 2331.04] loss=1.58 avg=1.91\n",
            "Saving checkpoint/run1/model-1000\n",
            "WARNING:tensorflow:From /tensorflow-1.15.2/python3.7/tensorflow_core/python/training/saver.py:963: remove_checkpoint (from tensorflow.python.training.checkpoint_management) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use standard file APIs to delete files with this prefix.\n",
            "======== SAMPLE 1 ========\n",
            "PractualSQLList project with YOLO - using it for training our perceptron models, and then comparing the models. The final model will be announced after extensive study!https://t.co/YpB4N1lLhK<|endoftext|>\n",
            "<|startoftext|>Day 25 of #66DaysOfData with @KenJee_DS.Continued with the Housing Dataset from Kaggle. Brief overviews of the software in SQL.Also watched the video of @KenJee_DS \"A brief introduction to data science project\". Interesting video.<|endoftext|>\n",
            "<|startoftext|>Day 23 of #66daysofdata Today I had time to finish the Kaggle course, Linear Discriminant Analysis (LDA). https://t.co/nhxK2h2n7B<|endoftext|>\n",
            "<|startoftext|>Day 25 of #66daysofdata -I revised my Statistics for Data Science Kaggle lesson 2, learned about the basic intuition behind the lesson.  I went through several lessons on SVM and Naive Bayes Classifier.<|endoftext|>\n",
            "<|startoftext|>Day 25 of #66DaysOfDataSQL Revisit: Types of Sub-Strings and Strings in SQL Server@KenJee_DShttps://t.co/3Wt2LlW5j3<|endoftext|>\n",
            "<|startoftext|>Day 12 of #66daysofdata :Didn't do much today, I worked on a kaggle project for the time being and then I watched a couple of videos on https://t.co/mCfj2L0aN<|endoftext|>\n",
            "<|startoftext|>Day 24- Watched some video tutorial video in @kaggle- went through examples of how to use @emilykinsey 's clever storytelling video to refresh your memory about data analysis. https://t.co/j3hLJwX5vJ<|endoftext|>\n",
            "<|startoftext|>Day 24 V_2 of #66DaysOfData with @KenJee_DS(2) :- Watched and learned 2 videos from the https://t.co/Jw2o2VyqT video- Started with @HackerRank and followed by @JackRaifer's (and others') \"Hacking the App Market\" serieshttps://t.co/5Bc0x2Td4U<|endoftext|>\n",
            "<|startoftext|>Day 18 of #66daysofdata by @KenJee_DS started with  Dimensionality Reduction. This is a pretty cool application of Linear Classifier to improve a model's precision at low dimensions (less than 0.1). https://t.co/2lLmTfSqLh<|endoftext|>\n",
            "<|startoftext|>Day 12 of #66daysofdata - Today I continued study about feature scaling. The topic is: How much should a given feature be chosen from the image?<|endoftext|>\n",
            "<|startoftext|>day 18: As someone who's always looking at teaching small classes in my pet project, I couldn't think of any better way to start #66daysofdata. So I am creating a template for the above and the other 25 pages.#66DaysOfData<|endoftext|>\n",
            "<|startoftext|>10/66 #66daysofdata  Hi Everyone!I read a few of Chapter 2 (Hands-on Machine Learning with Scikit-Learn, TensorFlow) published by @OReillyMedia on @Medium . Very informative for the people who want to learn something from reading it in full. https://t.co/g3i8eHkqjF<|endoftext|>\n",
            "<|startoftext|>What is the most important type of feature for Image Import in Python? #100DaysOfCode #100DaysOfMLCode #100daysofcode #66daysofdata #MachineLearning #CodeNewbies #code #DigitalEngineering #code #100DaysOfCode #100DaysOfMLCode #Python #DEVCommunity #100DaysOfCode #AI #computerscience #ArtificialIntelligence #webdev https://t.co/fJfzQZjbVc<|endoftext|>\n",
            "<|startoftext|>Day 21 of #66daysofdataContinued my udemy python course. Learning more about Data Manipulation with Pandas and Data Comprehensions from @DataCamp<\n",
            "\n",
            "[1010 | 2365.62] loss=1.39 avg=1.90\n",
            "[1020 | 2388.76] loss=1.37 avg=1.89\n",
            "[1030 | 2411.88] loss=1.48 avg=1.89\n",
            "[1040 | 2434.98] loss=1.41 avg=1.88\n",
            "[1050 | 2458.08] loss=1.45 avg=1.87\n",
            "[1060 | 2481.16] loss=1.29 avg=1.86\n",
            "[1070 | 2504.21] loss=1.43 avg=1.86\n",
            "[1080 | 2527.28] loss=1.52 avg=1.85\n",
            "[1090 | 2550.35] loss=1.52 avg=1.85\n",
            "[1100 | 2573.44] loss=1.56 avg=1.84\n",
            "[1110 | 2596.54] loss=1.23 avg=1.83\n",
            "[1120 | 2619.67] loss=1.04 avg=1.82\n",
            "[1130 | 2642.79] loss=1.46 avg=1.82\n",
            "[1140 | 2665.89] loss=1.42 avg=1.81\n",
            "[1150 | 2688.98] loss=1.31 avg=1.80\n",
            "[1160 | 2712.04] loss=1.29 avg=1.80\n",
            "[1170 | 2735.11] loss=1.50 avg=1.79\n",
            "[1180 | 2758.17] loss=1.47 avg=1.79\n",
            "[1190 | 2781.24] loss=1.13 avg=1.78\n",
            "[1200 | 2804.32] loss=1.21 avg=1.77\n",
            "[1210 | 2827.42] loss=1.18 avg=1.76\n",
            "[1220 | 2850.53] loss=1.36 avg=1.76\n",
            "[1230 | 2873.65] loss=1.21 avg=1.75\n",
            "[1240 | 2896.78] loss=1.23 avg=1.74\n",
            "[1250 | 2919.91] loss=1.22 avg=1.73\n",
            "[1260 | 2943.08] loss=1.09 avg=1.73\n",
            "[1270 | 2966.22] loss=1.32 avg=1.72\n",
            "[1280 | 2989.35] loss=1.18 avg=1.71\n",
            "[1290 | 3012.46] loss=1.07 avg=1.70\n",
            "[1300 | 3035.55] loss=1.27 avg=1.70\n",
            "[1310 | 3058.63] loss=1.04 avg=1.69\n",
            "[1320 | 3081.69] loss=1.15 avg=1.68\n",
            "[1330 | 3104.76] loss=1.03 avg=1.67\n",
            "[1340 | 3127.82] loss=1.34 avg=1.67\n",
            "[1350 | 3150.90] loss=1.17 avg=1.66\n",
            "[1360 | 3174.01] loss=1.20 avg=1.65\n",
            "[1370 | 3197.15] loss=0.89 avg=1.64\n",
            "[1380 | 3220.30] loss=1.14 avg=1.64\n",
            "[1390 | 3243.44] loss=1.09 avg=1.63\n",
            "[1400 | 3266.57] loss=1.16 avg=1.62\n",
            "[1410 | 3289.70] loss=1.05 avg=1.62\n",
            "[1420 | 3312.82] loss=1.18 avg=1.61\n",
            "[1430 | 3335.96] loss=0.96 avg=1.60\n",
            "[1440 | 3359.06] loss=0.82 avg=1.59\n",
            "[1450 | 3382.17] loss=1.03 avg=1.58\n",
            "[1460 | 3405.24] loss=1.14 avg=1.58\n",
            "[1470 | 3428.30] loss=1.07 avg=1.57\n",
            "[1480 | 3451.38] loss=0.94 avg=1.56\n",
            "[1490 | 3474.45] loss=0.81 avg=1.55\n",
            "[1500 | 3497.52] loss=0.86 avg=1.55\n",
            "Saving checkpoint/run1/model-1500\n",
            "======== SAMPLE 1 ========\n",
            ", \"How to learn Python with Data Science\"https://t.co/aDU00s6J3f#DataScience #MachineLearning #Python #DataScience<|endoftext|>\n",
            "<|startoftext|>Day 9 of #66DaysOfData! \"Machine Learning\" by AndrewYNg on #Coursera.Part of the course:- Week5/Regularization: Logistic regression, decision trees and random forest.<|endoftext|>\n",
            "<|startoftext|>Day 3: 2nd round #66daysofdata - completed project of analysis - covered next ML algorithms- started EDA - still not done with it but interested to learn more about web scrapingIt's probably not the most interesting topic, but I can say it's important.https://t.co/A7sS2jEeZn<|endoftext|>\n",
            "<|startoftext|>Day 7: 2nd round #66daysofdata - completed project of data visualization on @HackerRank It's not sure if I'll continue, but i'm excited to try and watch how it goes!<|endoftext|>\n",
            "<|startoftext|>Day 3 of #66daysofdata.I used the time to brush up on some statistics.I learnt about the distributions and different means of doing the EDA. I also interacted with some datasets and played around with them.<|endoftext|>\n",
            "<|startoftext|>Day 3 of #66DaysOfData:- Started 'Data Analytics' by Jose Portilla- Learned more about Python in Private Quantopian<|endoftext|>\n",
            "<|startoftext|>I missed yesterday so I'm on Day 2 now!- Participated in #RandomIntelligence Experiment (SKU) on #kaggle using the COLLECTION fundai dataset on #kagglehttps://t.co/QmWqxW7R6I #DataScience  #DataAnalytics #DataScientist #66daysofdata  v2 @KenJee_DShttps://t.co/4s2WK7ztJZ<|endoftext|>\n",
            "<|startoftext|>Day 1 of #66daysofdata : Reviewing Multiple Regression: R2- Regression Model Read more about R2- Regression Model on @Codecademy https://t.co/P2NTb0RlSj.  This time around, I mainly learned about R2<|endoftext|>\n",
            "<|startoftext|>@KenJee_DS Thanks, I'll be happy if I re-post my progress too. I learned a lot still though! #66DaysOfData is officially over for me, because I managed to finish my #DataAnalytics book and start my Python journey.#DataScience#100DaysOfCode #Python #DataScientist #66daysofdata https://t.co/M6Vh1W9mM7<|endoftext|>\n",
            "<|startoftext|>Day 3 of #66daysofdata:Completed the week in a few #nflfastR theory sessions with @CoreyMSchafer. Never thought I'd be able to make something like this:1/3<|endoftext|>\n",
            "<|startoftext|>Day 3 of #66daysofdata!Watched a couple of videos on how to get a machine learning brush-on from @dataleaptech (https://t.co/Wd3PXg5tkq). Need to brush up on some basics in order to brush up on @zerotomasteryio and his amazing videos<|endoftext|>\n",
            "<|startoftext|>#66DaysOfData with @KenJee_DS round 2 day #28 and round 2 day #31:-&gt; Gave my day job interview as a data analyst and also went on a Data Engineering Explained podcast vid @KenJee_DS.It's definitely refreshing to go through his journey and hear his journey-level challenges and challenges that he has gone through.https://t.co/IgSIGJW3B3<|endoftext|>\n",
            "<|startoftext|>Day 27 of #66daysofdataStarted working on an assignment for a customer research organization.  Implementing Machine Learning Algorithm utilizing Ridge and Bayesian methods.#datascience #machinelearning #python<|endoftext|>\n",
            "<|startoftext|>Day 28: ML with R #66daysofdata<|endoftext|>\n",
            "<|startoftext|>Day 20 of #66daysofdata R2- Today was a light session.\n",
            "\n",
            "[1510 | 3532.13] loss=1.29 avg=1.54\n",
            "[1520 | 3555.28] loss=0.85 avg=1.53\n",
            "[1530 | 3578.41] loss=0.91 avg=1.53\n",
            "[1540 | 3601.52] loss=0.84 avg=1.52\n",
            "[1550 | 3624.59] loss=0.60 avg=1.51\n",
            "[1560 | 3647.65] loss=0.90 avg=1.50\n",
            "[1570 | 3670.71] loss=0.87 avg=1.49\n",
            "[1580 | 3693.75] loss=0.85 avg=1.48\n",
            "[1590 | 3716.81] loss=0.93 avg=1.47\n",
            "[1600 | 3739.86] loss=0.99 avg=1.47\n",
            "[1610 | 3762.97] loss=1.02 avg=1.46\n",
            "[1620 | 3786.09] loss=0.81 avg=1.46\n",
            "[1630 | 3809.19] loss=0.72 avg=1.45\n",
            "[1640 | 3832.31] loss=1.12 avg=1.44\n",
            "[1650 | 3855.46] loss=0.85 avg=1.43\n",
            "[1660 | 3878.61] loss=1.03 avg=1.43\n",
            "[1670 | 3901.78] loss=0.78 avg=1.42\n",
            "[1680 | 3924.93] loss=0.83 avg=1.41\n",
            "[1690 | 3948.08] loss=0.84 avg=1.41\n",
            "[1700 | 3971.21] loss=0.75 avg=1.40\n",
            "[1710 | 3994.35] loss=0.75 avg=1.39\n",
            "[1720 | 4017.48] loss=1.01 avg=1.39\n",
            "[1730 | 4040.62] loss=0.87 avg=1.38\n",
            "[1740 | 4063.73] loss=0.80 avg=1.37\n",
            "[1750 | 4086.85] loss=0.99 avg=1.37\n",
            "[1760 | 4109.98] loss=0.91 avg=1.36\n",
            "[1770 | 4133.09] loss=0.61 avg=1.35\n",
            "[1780 | 4156.20] loss=0.86 avg=1.35\n",
            "[1790 | 4179.31] loss=0.65 avg=1.34\n",
            "[1800 | 4202.42] loss=0.88 avg=1.33\n",
            "[1810 | 4225.53] loss=0.71 avg=1.33\n",
            "[1820 | 4248.67] loss=0.63 avg=1.32\n",
            "[1830 | 4271.81] loss=0.69 avg=1.31\n",
            "[1840 | 4294.94] loss=0.81 avg=1.31\n",
            "[1850 | 4318.06] loss=0.65 avg=1.30\n",
            "[1860 | 4341.17] loss=0.76 avg=1.29\n",
            "[1870 | 4364.30] loss=0.76 avg=1.28\n",
            "[1880 | 4387.44] loss=0.60 avg=1.28\n",
            "[1890 | 4410.57] loss=0.62 avg=1.27\n",
            "[1900 | 4433.67] loss=0.64 avg=1.26\n",
            "[1910 | 4456.77] loss=0.61 avg=1.25\n",
            "[1920 | 4479.89] loss=0.77 avg=1.25\n",
            "[1930 | 4502.99] loss=0.72 avg=1.24\n",
            "[1940 | 4526.09] loss=0.56 avg=1.23\n",
            "[1950 | 4549.22] loss=0.75 avg=1.23\n",
            "[1960 | 4572.34] loss=0.83 avg=1.22\n",
            "[1970 | 4595.46] loss=0.53 avg=1.22\n",
            "[1980 | 4618.57] loss=0.55 avg=1.21\n",
            "[1990 | 4641.72] loss=0.58 avg=1.20\n",
            "[2000 | 4664.85] loss=0.72 avg=1.20\n",
            "Saving checkpoint/run1/model-2000\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "u9KLqVeyIF2o",
        "outputId": "b47e426e-e2a1-4cd0-d37a-9e5cfac99039"
      },
      "source": [
        "# Zip run in checkpoint folder\n",
        "!zip -r /content/run1.zip /content/checkpoint/run1"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "  adding: content/checkpoint/run1/ (stored 0%)\n",
            "  adding: content/checkpoint/run1/model-2000.data-00000-of-00001 (deflated 7%)\n",
            "  adding: content/checkpoint/run1/model-2000.meta (deflated 91%)\n",
            "  adding: content/checkpoint/run1/counter (stored 0%)\n",
            "  adding: content/checkpoint/run1/model-2000.index (deflated 62%)\n",
            "  adding: content/checkpoint/run1/events.out.tfevents.1619339169.0cf9f22a178c (deflated 61%)\n",
            "  adding: content/checkpoint/run1/encoder.json (deflated 67%)\n",
            "  adding: content/checkpoint/run1/checkpoint (deflated 40%)\n",
            "  adding: content/checkpoint/run1/vocab.bpe (deflated 53%)\n",
            "  adding: content/checkpoint/run1/hparams.json (deflated 28%)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xSPHf7z6cm_z",
        "outputId": "f98c3546-de88-472d-a5ee-0756d6251c1b"
      },
      "source": [
        "!unzip /content/run1.zip"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Archive:  /content/run1.zip\n",
            "   creating: content/checkpoint/run1/\n",
            "  inflating: content/checkpoint/run1/model-2000.data-00000-of-00001  \n",
            "  inflating: content/checkpoint/run1/model-2000.meta  \n",
            " extracting: content/checkpoint/run1/counter  \n",
            "  inflating: content/checkpoint/run1/model-2000.index  \n",
            "  inflating: content/checkpoint/run1/events.out.tfevents.1619339169.0cf9f22a178c  \n",
            "  inflating: content/checkpoint/run1/encoder.json  \n",
            "  inflating: content/checkpoint/run1/checkpoint  \n",
            "  inflating: content/checkpoint/run1/vocab.bpe  \n",
            "  inflating: content/checkpoint/run1/hparams.json  \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MTth9AqEbsyv",
        "outputId": "2ad9823d-87fa-471e-c2f5-eddac264f4f4"
      },
      "source": [
        "gpt2.generate(sess, run_name='run1')"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Ken Jee‚Äôs next step might be to start a data science book.  Not gonna lie. It's gonna take time.  But, ya, ya! :) #66daysofdata<|endoftext|>\n",
            "<|startoftext|>Day 25 of #66daysofdata:I've completed the Introduction to the Tidyverse course on Dataquest. It's one of my favorite parts of the course, it's good people I know and use Text Analysis and Data Wrangling for data science.<|endoftext|>\n",
            "<|startoftext|>Day 3 of #66daysofdata :I've completed the Pandas course on Kaggle! https://t.co/Zq2ubw2kYM<|endoftext|>\n",
            "<|startoftext|>Day 25: I had watched: Tutorial for Understanding and Visualizing Machine Learning in Python: https://t.co/yVZ9qJit2F #66daysofdata #datascience https://t.co/Zj7m9Z5zPs<|endoftext|>\n",
            "<|startoftext|>Day 25 of #66daysofdata:I finished the kaggle python course. I‚Äôm also going to start a data science one :)<|endoftext|>\n",
            "<|startoftext|>Day 25 of #66daysofdataSQL Revisit:SQL statements are not required when working with large strings. (char, aggregate, timestamp)‚Ä¢ working with large strings in excel‚Ä¢ using a dict to organise values in the dataframe‚Ä¢ working with large strings in python<|endoftext|>\n",
            "<|startoftext|>Day 6 of #66daysofdataRead Chapter 3 of \"Hands-On Machine Learning\" which explored the use of RNNs and LSTMs in classification problems. @KenJee_DS<|endoftext|>\n",
            "<|startoftext|>Day 25 of #66daysofdata: Classification performance of COVID-19 confirmed that the model is correct. The model predictions are pretty good, even with multiple features in the model. The main issue is that the training time is shorter than the test time.#DataScience #MachineLearning https://t.co/Ibd8Cj9MpT<|endoftext|>\n",
            "<|startoftext|>Day 25 of #66daysofdata: Today, I started the second course of the Tensorflow Developer Certificate Specialisation @coursera, and volunteered for Houston Food Bank for ~ 3 hours while everybody is watching elections (:<|endoftext|>\n",
            "<|startoftext|>#100DaysOfCode #66daysofdata Day 1 (Sept 25) : - Spent about an hour learning data structures.- Learnt about NLP and computational vision.- Worked on a notebook about numerai (hours to build it).<|endoftext|>\n",
            "<|startoftext|>Day 25 of #66daysofdata: I started the 'Feature Engineering' course on Kaggle. I'm looking forward to applying this knowledge into my projects!<|endoftext|>\n",
            "<|startoftext|>Day 25 of #66daysofdata:Finished \"Intro to importing SQL data\" on Data Camp.<|endoftext|>\n",
            "<|startoftext|>Day 25 of #66daysofdata:  Today, I started  SQL Zoo, the first supervised learning model in which I will learn and apply some of the supervised learning principles from the book. @KenJee_DS<|endoftext|>\n",
            "<|startoftext|>Day 25 of #66daysofdata: Today, I finished Kaggle's Geospatial Analysis mini-course, I completed the assignment \"Using OOPs in Kaggle Compose\". It's a good course summary and a good reason to take this #DataScience course. https://t.co/YsAZqrteLQ<|endoftext|>\n",
            "<|startoftext|>Day 9 of #66DaysOfDataCompleted the Boolean and Conditional probability modules in the Data Structures and Algorithms courseAdded code to my in-built functions in my functions.hs book. PS: Now I can use all the theory that I learnt working with LSTM to my advantage.Book Link: https://t.co/z9aLt8YANN https://t.co/q3qV5qjsC<|endoftext|>\n",
            "<|startoftext|>Day 12 of #66daysofdata: Today, I finished the Boolean and Conditional probability modules in the Data Structures and Algorithms course on @cour\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2MlWtTL1cRuq"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}